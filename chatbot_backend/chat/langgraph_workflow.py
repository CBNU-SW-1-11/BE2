# langgraph_workflow.py - ìƒˆë¡œ ìƒì„±í•  íŒŒì¼
from langgraph.graph import StateGraph, END
from typing import TypedDict, List, Dict, Any, Annotated
from langchain.schema import BaseMessage
import asyncio
import logging
import time
import json
from .langchain_config import LangChainManager, GroqLLM

logger = logging.getLogger(__name__)

# State ì •ì˜
class AIResponseState(TypedDict):
    user_message: str
    user_language: str
    selected_models: List[str]
    request_id: str
    
    # ê° ë‹¨ê³„ë³„ ê²°ê³¼
    individual_responses: Dict[str, str]
    similarity_analysis: Dict[str, Any]
    final_analysis: Dict[str, Any]
    
    # ì—ëŸ¬ ì²˜ë¦¬
    errors: List[str]
    current_step: str

# ì›Œí¬í”Œë¡œìš° í´ë˜ìŠ¤
class AIComparisonWorkflow:
    def __init__(self, langchain_manager: LangChainManager, similarity_analyzer):
        self.langchain_manager = langchain_manager
        self.similarity_analyzer = similarity_analyzer
        self.groq_llm = GroqLLM(langchain_manager.groq_key)
        
        # ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±
        self.workflow = self.create_workflow()
    
    def create_workflow(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        workflow = StateGraph(AIResponseState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("collect_responses", self.collect_ai_responses)
        workflow.add_node("analyze_similarity", self.analyze_response_similarity)
        workflow.add_node("generate_optimal", self.generate_optimal_response)
        workflow.add_node("handle_error", self.handle_workflow_error)
        
        # ì—£ì§€ ì¶”ê°€ (ì›Œí¬í”Œë¡œìš° ìˆœì„œ)
        workflow.set_entry_point("collect_responses")
        
        # ì¡°ê±´ë¶€ ë¼ìš°íŒ…
        workflow.add_conditional_edges(
            "collect_responses",
            self.should_continue_after_collection,
            {
                "continue": "analyze_similarity",
                "error": "handle_error"
            }
        )
        
        workflow.add_conditional_edges(
            "analyze_similarity", 
            self.should_continue_after_similarity,
            {
                "continue": "generate_optimal",
                "error": "handle_error"
            }
        )
        
        workflow.add_edge("generate_optimal", END)
        workflow.add_edge("handle_error", END)
        
        return workflow.compile()
    
    async def collect_ai_responses(self, state: AIResponseState) -> AIResponseState:
        """1ë‹¨ê³„: ê° AI ëª¨ë¸ë¡œë¶€í„° ì‘ë‹µ ìˆ˜ì§‘"""
        logger.info("ğŸ¤– 1ë‹¨ê³„: AI ì‘ë‹µ ìˆ˜ì§‘ ì‹œì‘")
        state["current_step"] = "collecting_responses"
        
        responses = {}
        errors = []
        
        # selectedModelsê°€ ì˜¬ë°”ë¥¸ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
        selected_models = state["selected_models"]
        if isinstance(selected_models, str):
            logger.error(f"âŒ selected_modelsê°€ ë¬¸ìì—´ì…ë‹ˆë‹¤: {selected_models}")
            try:
                import json
                selected_models = json.loads(selected_models)
                logger.info(f"âœ… JSON íŒŒì‹± ì„±ê³µ: {selected_models}")
            except:
                logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
                selected_models = ['gpt', 'claude', 'mixtral']
        
        logger.info(f"ğŸ¯ ì²˜ë¦¬í•  ëª¨ë¸ ëª©ë¡: {selected_models} (íƒ€ì…: {type(selected_models)})")
        
        # ë³‘ë ¬ë¡œ ê° ëª¨ë¸ì—ì„œ ì‘ë‹µ ìˆ˜ì§‘
        tasks = []
        for model in selected_models:
            logger.info(f"ğŸ“¡ {model} ëª¨ë¸ ì‘ë‹µ ìš”ì²­ ì¤€ë¹„ ì¤‘...")
            
            if model in ['gpt', 'claude', 'mixtral']:  # ëª¨ë“  ëª¨ë¸ì´ LangChain í†µí•©ë¨
                task = self.get_langchain_response(
                    model, 
                    state["user_message"], 
                    state["user_language"]
                )
            else:
                # ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì˜ ê²½ìš° ì—ëŸ¬ ë°œìƒ
                error_msg = f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model}"
                errors.append(error_msg)
                logger.error(f"âŒ {error_msg}")
                continue
            tasks.append((model, task))
        
        logger.info(f"ğŸš€ {len(tasks)}ê°œ ëª¨ë¸ ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘")
        
        # ë¹„ë™ê¸° ì‹¤í–‰
        for model, task in tasks:
            try:
                logger.info(f"â³ {model} ì‘ë‹µ ëŒ€ê¸° ì¤‘...")
                response = await task
                responses[model] = response
                logger.info(f"âœ… {model} ì‘ë‹µ ì™„ë£Œ: {response[:50]}...")
            except Exception as e:
                error_msg = f"{model} ì‘ë‹µ ì‹¤íŒ¨: {str(e)}"
                errors.append(error_msg)
                logger.error(error_msg)
        
        state["individual_responses"] = responses
        state["errors"].extend(errors)
        
        logger.info(f"ğŸ“Š ì‘ë‹µ ìˆ˜ì§‘ ì™„ë£Œ: {len(responses)}ê°œ ëª¨ë¸, {len(errors)}ê°œ ì˜¤ë¥˜")
        return state
    
    async def get_langchain_response(self, model: str, message: str, language: str) -> str:
        """LangChainì„ í†µí•œ ì‘ë‹µ íšë“"""
        try:
            chain = self.langchain_manager.create_chat_chain(model)
            if model == 'gpt':
                # ChatOpenAIëŠ” ë¹„ë™ê¸° ì§€ì›
                result = await chain.arun(
                    user_input=message,
                    user_language=language
                )
            else:
                # ì»¤ìŠ¤í…€ LLMë“¤ì€ ë™ê¸° ë°©ì‹ ì‚¬ìš©
                result = chain.run(
                    user_input=message,
                    user_language=language
                )
            return result
        except Exception as e:
            logger.error(f"LangChain {model} ì—ëŸ¬: {e}")
            raise
    
    async def get_groq_response(self, message: str, language: str) -> str:
        """Groqì„ í†µí•œ ì‘ë‹µ íšë“ (ì´ì œ LangChain í†µí•©ë¨)"""
        try:
            return await self.get_langchain_response('mixtral', message, language)
        except Exception as e:
            logger.error(f"Groq ì—ëŸ¬: {e}")
            raise
    
    async def analyze_response_similarity(self, state: AIResponseState) -> AIResponseState:
        """2ë‹¨ê³„: ì‘ë‹µ ìœ ì‚¬ë„ ë¶„ì„"""
        logger.info("ğŸ” 2ë‹¨ê³„: ìœ ì‚¬ë„ ë¶„ì„ ì‹œì‘")
        state["current_step"] = "analyzing_similarity"
        
        try:
            responses = state["individual_responses"]
            if len(responses) >= 2:
                similarity_result = self.similarity_analyzer.cluster_responses(responses)
                state["similarity_analysis"] = self.convert_to_serializable(similarity_result)
                logger.info("âœ… ìœ ì‚¬ë„ ë¶„ì„ ì™„ë£Œ")
            else:
                state["similarity_analysis"] = {}
                logger.warning("âš ï¸ ìœ ì‚¬ë„ ë¶„ì„ì„ ìœ„í•œ ì¶©ë¶„í•œ ì‘ë‹µì´ ì—†ìŒ")
        except Exception as e:
            error_msg = f"ìœ ì‚¬ë„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            state["errors"].append(error_msg)
            logger.error(error_msg)
        
        return state
    
    async def generate_optimal_response(self, state: AIResponseState) -> AIResponseState:
        """3ë‹¨ê³„: ìµœì  ì‘ë‹µ ìƒì„±"""
        logger.info("âœ¨ 3ë‹¨ê³„: ìµœì  ì‘ë‹µ ìƒì„± ì‹œì‘")
        state["current_step"] = "generating_optimal"
        
        try:
            responses = state["individual_responses"]
            selected_models = list(responses.keys())
            
            # ë¶„ì„ì— ì‚¬ìš©í•  ëª¨ë¸ ê²°ì • (ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ ì²« ë²ˆì§¸ ì‚¬ìš©)
            analyzer_model = 'gpt'  # ê¸°ë³¸ê°’
            if 'gpt' in selected_models:
                analyzer_model = 'gpt'
            elif 'claude' in selected_models:
                analyzer_model = 'claude'
            elif 'mixtral' in selected_models:
                analyzer_model = 'mixtral'
            
            # LangChain ë¶„ì„ ì²´ì¸ ì‚¬ìš©
            analysis_chain = self.langchain_manager.create_analysis_chain(analyzer_model)
            
            # ì‘ë‹µ í¬ë§·íŒ…
            formatted = self.langchain_manager.format_responses_for_analysis(
                responses, selected_models
            )
            
            # ìµœì¢… ë¶„ì„ ì‹¤í–‰
            if analyzer_model == 'gpt':
                # ChatOpenAIëŠ” ë¹„ë™ê¸° ì§€ì›
                analysis_result = await analysis_chain.arun(
                    query=state["user_message"],
                    user_language=state["user_language"],
                    selected_models=selected_models,
                    **formatted
                )
            else:
                # ì»¤ìŠ¤í…€ LLMë“¤ì€ ë™ê¸° ë°©ì‹
                analysis_result = analysis_chain.run(
                    query=state["user_message"],
                    user_language=state["user_language"],
                    selected_models=selected_models,
                    **formatted
                )
            
            state["final_analysis"] = analysis_result
            logger.info("âœ… ìµœì  ì‘ë‹µ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            error_msg = f"ìµœì  ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}"
            state["errors"].append(error_msg)
            logger.error(error_msg)
            
            # í´ë°± ì‘ë‹µ ìƒì„±
            state["final_analysis"] = self.create_fallback_response(
                state["individual_responses"], 
                selected_models
            )
        
        return state
    
    async def handle_workflow_error(self, state: AIResponseState) -> AIResponseState:
        """ì—ëŸ¬ ì²˜ë¦¬"""
        logger.error(f"ğŸš¨ ì›Œí¬í”Œë¡œìš° ì—ëŸ¬ ì²˜ë¦¬: {state['errors']}")
        state["current_step"] = "error_handled"
        
        # ê°€ëŠ¥í•œ ë¶€ë¶„ì  ê²°ê³¼ë¼ë„ ë°˜í™˜
        if not state.get("final_analysis"):
            state["final_analysis"] = self.create_fallback_response(
                state.get("individual_responses", {}),
                state["selected_models"]
            )
        
        return state
    
    def should_continue_after_collection(self, state: AIResponseState) -> str:
        """ì‘ë‹µ ìˆ˜ì§‘ í›„ ê³„ì†í• ì§€ ê²°ì •"""
        if len(state["individual_responses"]) == 0:
            return "error"
        return "continue"
    
    def should_continue_after_similarity(self, state: AIResponseState) -> str:
        """ìœ ì‚¬ë„ ë¶„ì„ í›„ ê³„ì†í• ì§€ ê²°ì •"""
        # ìœ ì‚¬ë„ ë¶„ì„ì´ ì‹¤íŒ¨í•´ë„ ìµœì  ì‘ë‹µ ìƒì„±ì€ ì§„í–‰
        return "continue"
    
    def convert_to_serializable(self, obj):
        """ê°ì²´ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
        if hasattr(obj, '__dict__'):
            return {k: self.convert_to_serializable(v) for k, v in obj.__dict__.items()}
        elif isinstance(obj, dict):
            return {k: self.convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self.convert_to_serializable(item) for item in obj]
        else:
            return obj
    
    def create_fallback_response(self, responses: Dict[str, str], selected_models: List[str]) -> Dict[str, Any]:
        """í´ë°± ì‘ë‹µ ìƒì„±"""
        best_response = ""
        if responses:
            best_response = max(responses.values(), key=len)
        
        error_analysis = {}
        for model in selected_models:
            model_lower = model.lower()
            error_analysis[model_lower] = {
                "ì¥ì ": "ë¶„ì„ ì‹¤íŒ¨", 
                "ë‹¨ì ": "ë¶„ì„ ì‹¤íŒ¨"
            }
        
        return {
            "preferredModel": "FALLBACK",
            "best_response": best_response,
            "analysis": error_analysis,
            "reasoning": "ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ í´ë°± ì‘ë‹µì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤."
        }
    
    async def run_workflow(self, user_message: str, selected_models: List[str], 
                          user_language: str = 'ko', request_id: str = None) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        if not request_id:
            request_id = str(time.time())
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = AIResponseState(
            user_message=user_message,
            user_language=user_language,
            selected_models=selected_models,
            request_id=request_id,
            individual_responses={},
            similarity_analysis={},
            final_analysis={},
            errors=[],
            current_step="initialized"
        )
        
        logger.info(f"ğŸš€ ì›Œí¬í”Œë¡œìš° ì‹œì‘ - Request ID: {request_id}")
        
        try:
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            final_state = await self.workflow.ainvoke(initial_state)
            
            logger.info(f"ğŸ‰ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ - Request ID: {request_id}")
            
            return {
                "request_id": request_id,
                "individual_responses": final_state["individual_responses"],
                "similarity_analysis": final_state["similarity_analysis"],
                "final_analysis": final_state["final_analysis"],
                "errors": final_state["errors"],
                "final_step": final_state["current_step"]
            }
            
        except Exception as e:
            logger.error(f"ğŸš¨ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "request_id": request_id,
                "individual_responses": {},
                "similarity_analysis": {},
                "final_analysis": self.create_fallback_response({}, selected_models),
                "errors": [str(e)],
                "final_step": "workflow_failed"
            }