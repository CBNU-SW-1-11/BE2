# video_analyzer.py - ìƒë‹¨ import ë° ì´ˆê¸°í™” ë¶€ë¶„ ìˆ˜ì •
import os
import json
import numpy as np
import cv2
import colorsys
from collections import Counter, defaultdict
from datetime import datetime
import torch
import torch.nn as nn
import time
from PIL import Image
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from dotenv import load_dotenv
import threading
# í™˜ê²½ ì„¤ì •
load_dotenv()  
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
# API í´ë¼ì´ì–¸íŠ¸ë“¤
from groq import Groq
import openai
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    print("â„¹ï¸ Anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ - Anthropic API ë¹„í™œì„±í™”")

# AI ëª¨ë¸ë“¤ - ì•ˆì „í•œ import ì²˜ë¦¬
YOLO_AVAILABLE = False
TRANSFORMERS_AVAILABLE = False
OCR_AVAILABLE = False
MEDIAPIPE_AVAILABLE = False
NETWORKX_AVAILABLE = False

try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
    print("âœ… YOLO (Ultralytics) ë¡œë“œ ì„±ê³µ")
except ImportError:
    print("âš ï¸ YOLO (Ultralytics) ë¯¸ì„¤ì¹˜ - ê°ì²´ ê°ì§€ ê¸°ëŠ¥ ì œí•œë¨")
    print("ğŸ’¡ ì„¤ì¹˜ ë°©ë²•: pip install ultralytics")

try:
    from transformers import (
        BlipProcessor, BlipForConditionalGeneration, BlipForQuestionAnswering,
        CLIPProcessor, CLIPModel, AutoTokenizer, AutoModelForCausalLM, pipeline
    )
    TRANSFORMERS_AVAILABLE = True
    print("âœ… Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError:
    print("âš ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ - VQA/CLIP ê¸°ëŠ¥ ë¹„í™œì„±í™”")

try:
    import easyocr
    OCR_AVAILABLE = True
    print("âœ… EasyOCR ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError:
    print("âš ï¸ EasyOCR ë¯¸ì„¤ì¹˜ - OCR ê¸°ëŠ¥ ë¹„í™œì„±í™”")

try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
    print("âœ… MediaPipe ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError:
    print("âš ï¸ MediaPipe ë¯¸ì„¤ì¹˜ - í¬ì¦ˆ ì¶”ì • ê¸°ëŠ¥ ë¹„í™œì„±í™”")

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
    print("âœ… NetworkX ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError:
    print("âš ï¸ NetworkX ë¯¸ì„¤ì¹˜ - Scene Graph ê¸°ëŠ¥ ì œí•œë¨")

# PyTorch ì•ˆì „ ì„¤ì • (ìˆ˜ì •ë¨)
try:
    torch_version = torch.__version__
    if torch_version >= "2.6.0" and YOLO_AVAILABLE:
        try:
            import ultralytics.nn.tasks
            torch.serialization.add_safe_globals([
                ultralytics.nn.tasks.DetectionModel,
                ultralytics.nn.tasks.SegmentationModel,
                ultralytics.nn.tasks.ClassificationModel,
                ultralytics.nn.tasks.PoseModel,
                ultralytics.nn.tasks.OBBModel,
                ultralytics.nn.tasks.WorldModel
            ])
            print(f"âœ… PyTorch {torch_version} - YOLO ì•ˆì „ ê¸€ë¡œë²Œ ì„¤ì • ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì•ˆì „ ê¸€ë¡œë²Œ ì„¤ì • ê±´ë„ˆëœ€: {e}")
except Exception as e:
    print(f"âš ï¸ PyTorch ì´ˆê¸°í™” ê²½ê³ : {e}")

# LangChain ê´€ë ¨ import ì¶”ê°€ (ì°¸ê³  ëª¨ë¸ í†µí•©)
try:
    from langchain_community.document_loaders import JSONLoader
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain.schema import Document
    from langchain.retrievers import EnsembleRetriever
    from langchain_community.retrievers import BM25Retriever
    from langchain_openai import ChatOpenAI
    LANGCHAIN_AVAILABLE = True
    print("âœ… LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸ LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ - RAG ê¸°ëŠ¥ ë¹„í™œì„±í™”")

# API ì„¤ì •
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
groq_client = Groq(api_key=GROQ_API_KEY)

# OpenAI í´ë¼ì´ì–¸íŠ¸
try:
    openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
    OPENAI_AVAILABLE = True
except Exception as e:
    print(f"âš ï¸ OpenAI API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    openai_client = None
    OPENAI_AVAILABLE = False

# Anthropic í´ë¼ì´ì–¸íŠ¸
try:
    if ANTHROPIC_AVAILABLE:
        anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
    else:
        anthropic_client = None
except Exception as e:
    print(f"âš ï¸ Anthropic API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    anthropic_client = None

# ì°¸ê³  ëª¨ë¸ ì„¤ì • (í†µí•©ë¨)
CHUNK_SIZE = 256
CHUNK_OVERLAP = 128
EMBEDDING_MODEL = "intfloat/e5-large"
K = 3
LLM = "gemma2-9b-it"
MAX_NEW_TOKENS = 512

PROMPT_TEMPLATE = """
You are an AI visual assistant that can analyze video content and provide detailed insights.

Using the provided video analysis information, answer the user's question accurately.
Context from video analysis:
{context}

Question: {question}

Answer:
"""

# COCO ê°ì²´ í´ë˜ìŠ¤ (ê¸°ì¡´ê³¼ ë™ì¼)
ENHANCED_OBJECTS = {
    0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus',
    6: 'train', 7: 'truck', 8: 'boat', 15: 'cat', 16: 'dog', 17: 'horse',
    18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe',
    56: 'chair', 57: 'couch', 58: 'potted_plant', 59: 'bed', 60: 'dining_table',
    61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard',
    67: 'cell_phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink',
    72: 'refrigerator', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase',
    29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports_ball', 33: 'kite',
    34: 'baseball_bat', 35: 'baseball_glove', 36: 'skateboard', 37: 'surfboard',
    38: 'tennis_racket', 39: 'bottle', 40: 'wine_glass', 41: 'cup', 42: 'fork', 43: 'knife', 
    44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange',
    50: 'broccoli', 51: 'carrot', 52: 'hot_dog', 53: 'pizza', 54: 'donut', 55: 'cake',
    73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy_bear',
    78: 'hair_drier', 79: 'toothbrush', 9: 'traffic_light', 10: 'fire_hydrant',
    11: 'stop_sign', 12: 'parking_meter', 13: 'bench', 14: 'bird'
}

# ë¡œê·¸ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ê¸€ë¡œë²Œ ë³€ìˆ˜
_logged_messages = set()

def log_once(message, level="INFO"):
    """ì¤‘ë³µ ë¡œê·¸ ë°©ì§€"""
    if message not in _logged_messages:
        print(f"[{level}] {message}")
        _logged_messages.add(message)

def call_groq_llm_enhanced(prompt, system_prompt="", model="llama-3.1-8b-instant", max_retries=3):
    """ê°œì„ ëœ LLM í˜¸ì¶œ í•¨ìˆ˜ - Rate Limiting ê°•í™” ë° ë¡œê·¸ ì¤‘ë³µ ì œê±°"""
    
    # Rate Limitingì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
    if not hasattr(call_groq_llm_enhanced, 'last_call_time'):
        call_groq_llm_enhanced.last_call_time = 0
        call_groq_llm_enhanced.call_count = 0
    
    # ìš”ì²­ ê°„ê²© ì œì–´ (ìµœì†Œ 2ì´ˆ)
    current_time = time.time()
    time_since_last_call = current_time - call_groq_llm_enhanced.last_call_time
    if time_since_last_call < 2.0:
        sleep_time = 2.0 - time_since_last_call
        log_once(f"â±ï¸ Rate limiting: {sleep_time:.1f}ì´ˆ ëŒ€ê¸°", "DEBUG")
        time.sleep(sleep_time)
    
    call_groq_llm_enhanced.last_call_time = time.time()
    call_groq_llm_enhanced.call_count += 1
    
    # ë„ˆë¬´ ë§ì€ ìš”ì²­ì‹œ ê¸´ ëŒ€ê¸°
    if call_groq_llm_enhanced.call_count % 20 == 0:
        log_once("ğŸ”„ ëŒ€ëŸ‰ ìš”ì²­ ê°ì§€ - 10ì´ˆ ëŒ€ê¸°", "WARNING")
        time.sleep(10)
    
    # Groq ë¨¼ì € ì‹œë„ (ê¸°ì¡´ ë¡œì§)
    for attempt in range(max_retries):
        try:
            response = groq_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.6,
                max_tokens=512,
                top_p=0.8
            )
            content = response.choices[0].message.content.strip()
            
            # ì„±ê³µ ë¡œê·¸ëŠ” ì¤‘ìš”í•œ ê²½ìš°ë§Œ
            # if attempt > 0:  # ì¬ì‹œë„ í›„ ì„±ê³µí•œ ê²½ìš°ë§Œ ë¡œê·¸
            #     log_once(f"âœ… Groq API ì„±ê³µ (ì‹œë„ {attempt + 1})", "SUCCESS")
            # return content
            
        except Exception as e:
            if "429" in str(e) or "rate_limit" in str(e).lower():
                wait_time = min(2 ** attempt * 2, 30)  # ì§€ìˆ˜ì  ë°±ì˜¤í”„, ìµœëŒ€ 30ì´ˆ
                log_once(f"â³ Rate limit - {wait_time}ì´ˆ ëŒ€ê¸° (ì‹œë„ {attempt + 1}/{max_retries})", "WARNING")
                time.sleep(wait_time)
                continue
            else:
                log_once(f"âš ï¸ Groq API ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}", "ERROR")
                if attempt < max_retries - 1:
                    time.sleep(1)
                    continue
                else:
                    break

    # OpenAI fallback
    if OPENAI_AVAILABLE and openai_client:
        try:
            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.6,
                max_tokens=512
            )
            content = response.choices[0].message.content.strip()
            log_once("âœ… OpenAI API fallback ì„±ê³µ", "SUCCESS")
            return content
            
        except Exception as e:
            log_once(f"âŒ OpenAI APIë„ ì‹¤íŒ¨: {e}", "ERROR")
    
    # Anthropic fallback
    if ANTHROPIC_AVAILABLE and anthropic_client:
        try:
            response = anthropic_client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=512,
                temperature=0.6,
                system=system_prompt,
                messages=[{"role": "user", "content": prompt}]
            )
            content = response.content[0].text.strip()
            log_once("âœ… Anthropic API fallback ì„±ê³µ", "SUCCESS")
            return content
            
        except Exception as e:
            log_once(f"âŒ Anthropic APIë„ ì‹¤íŒ¨: {e}", "ERROR")
    
    # ëª¨ë“  API ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì‘ë‹µ
    log_once("ğŸ”„ ê¸°ë³¸ ì‘ë‹µ ìƒì„±...", "WARNING")
    
    if "ê°ì²´" in prompt or "object" in prompt.lower():
        return "í”„ë ˆì„ì—ì„œ ì—¬ëŸ¬ ê°ì²´ê°€ ê°ì§€ë˜ì—ˆìœ¼ë©°, ì´ë“¤ ê°„ì˜ ê³µê°„ì  ê´€ê³„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¥ë©´ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤."
    elif "íŠ¹ì§•" in prompt or "feature" in prompt.lower():
        return '{"activity_type": 0.5, "emotional_tone": 0.6, "complexity_level": 0.4, "interaction_level": 0.3, "visual_coherence": 0.7, "spatial_composition": 0.6, "temporal_context": 0.5, "social_context": 0.4}'
    elif "ìº¡ì…˜" in prompt or "caption" in prompt.lower():
        return "ë™ì˜ìƒ í”„ë ˆì„ì—ì„œ ë‹¤ì–‘í•œ ê°ì²´ë“¤ì´ ê°ì§€ë˜ê³  ìˆìœ¼ë©°, ì „ë°˜ì ìœ¼ë¡œ ì¼ìƒì ì¸ ì¥ë©´ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤."
    else:
        return f"API í˜¸ì¶œ ì‹¤íŒ¨ë¡œ ì¸í•œ ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì"

class ColorAnalyzer:
    """ê³ ê¸‰ ìƒ‰ìƒ ë¶„ì„ê¸°"""

    def __init__(self):
        self.color_ranges = {
            'red': [(0, 70, 50), (10, 255, 255), (170, 70, 50), (180, 255, 255)],
            'orange': [(11, 70, 50), (25, 255, 255)],
            'yellow': [(26, 70, 50), (35, 255, 255)],
            'green': [(36, 70, 50), (85, 255, 255)],
            'blue': [(86, 70, 50), (125, 255, 255)],
            'purple': [(126, 70, 50), (155, 255, 255)],
            'pink': [(156, 70, 50), (169, 255, 255)],
            'white': [(0, 0, 200), (180, 25, 255)],
            'black': [(0, 0, 0), (180, 255, 50)],
            'gray': [(0, 0, 51), (180, 25, 199)],
            'brown': [(8, 60, 20), (20, 255, 200)]
        }
    
    def extract_dominant_colors(self, frame, bbox, num_colors=3):
        """ë°”ìš´ë”© ë°•ìŠ¤ ë‚´ ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ"""
        try:
            h, w = frame.shape[:2]
            
            x1 = max(0, int(bbox[0] * w))
            y1 = max(0, int(bbox[1] * h))
            x2 = min(w, int(bbox[2] * w))
            y2 = min(h, int(bbox[3] * h))
            
            roi = frame[y1:y2, x1:x2]
            
            if roi.size == 0:
                return []
            
            target_size = min(100, max(roi.shape[:2]))
            roi_resized = cv2.resize(roi, (target_size, target_size))
            hsv = cv2.cvtColor(roi_resized, cv2.COLOR_BGR2HSV)
            
            color_percentages = {}
            
            for color_name, ranges in self.color_ranges.items():
                mask = np.zeros(hsv.shape[:2], dtype=np.uint8)
                
                if len(ranges) == 4:  # red ê²½ìš°
                    mask1 = cv2.inRange(hsv, np.array(ranges[0]), np.array(ranges[1]))
                    mask2 = cv2.inRange(hsv, np.array(ranges[2]), np.array(ranges[3]))
                    mask = cv2.bitwise_or(mask1, mask2)
                else:
                    mask = cv2.inRange(hsv, np.array(ranges[0]), np.array(ranges[1]))
                
                percentage = (np.sum(mask > 0) / (hsv.shape[0] * hsv.shape[1])) * 100
                color_percentages[color_name] = round(percentage, 1)
            
            significant_colors = [(color, pct) for color, pct in color_percentages.items() if pct >= 3.0]
            significant_colors.sort(key=lambda x: x[1], reverse=True)
            
            return significant_colors[:num_colors]
            
        except Exception as e:
            print(f"âš ï¸ ìƒ‰ìƒ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return []
    
    def get_color_description(self, colors):
        """ìƒ‰ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not colors:
            return "unknown"
        
        if len(colors) == 1:
            return colors[0][0]
        
        dominant_colors = [color[0] for color in colors if color[1] >= 10.0]
        
        if len(dominant_colors) == 1:
            return dominant_colors[0]
        elif len(dominant_colors) == 2:
            return f"{dominant_colors[0]}-{dominant_colors[1]}"
        elif len(dominant_colors) >= 3:
            return f"{dominant_colors[0]}-mixed"
        else:
            minor_colors = [color[0] for color in colors[:2]]
            return f"{minor_colors[0]}-{minor_colors[1]}" if len(minor_colors) == 2 else minor_colors[0]

class SceneClassifier:
    """Scene ë¶„ë¥˜ê¸° - ì‹¤ë‚´/ì‹¤ì™¸, ì‹œê°„ëŒ€, ë‚ ì”¨, í™œë™ ë“± ë¶„ë¥˜"""
    
    def __init__(self):
        self.clip_processor = None
        self.clip_model = None
        
        try:
            if TRANSFORMERS_AVAILABLE:
                print("ğŸï¸ Scene ë¶„ë¥˜ê¸° ë¡œë”© ì¤‘...")
                self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch16")
                self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch16")
                
                self.scene_templates = {
                    'location': ['indoor', 'outdoor', 'street', 'home'],
                    'time': ['day', 'night'],
                    'weather': ['sunny', 'cloudy', 'clear'],
                    'activity': ['walking', 'driving', 'sitting', 'standing']
                }
                
                print("ğŸï¸ Scene ë¶„ë¥˜ê¸° ë¡œë“œ ì™„ë£Œ")
            else:
                raise ImportError("Transformers not available")
                
        except Exception as e:
            print(f"âš ï¸ Scene ë¶„ë¥˜ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.scene_templates = {
                'location': ['indoor', 'outdoor'],
                'time': ['day', 'night'], 
                'activity': ['general']
            }
    
    def classify_scene(self, frame):
        """Scene ë¶„ë¥˜ ìˆ˜í–‰"""
        if not self.clip_processor or not self.clip_model:
            return self._basic_scene_classification(frame)
        
        try:
            image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            
            scene_classification = {}
            
            for category, labels in self.scene_templates.items():
                text_inputs = [f"a photo of {label}" for label in labels]
                
                inputs = self.clip_processor(
                    text=text_inputs, 
                    images=image, 
                    return_tensors="pt", 
                    padding=True
                )
                
                outputs = self.clip_model(**inputs)
                logits_per_image = outputs.logits_per_image
                probs = logits_per_image.softmax(dim=1)
                
                best_idx = probs.argmax().item()
                best_label = labels[best_idx]
                confidence = probs[0][best_idx].item()
                
                scene_classification[category] = {
                    'label': best_label,
                    'confidence': float(confidence),
                    'all_scores': {label: float(score) for label, score in zip(labels, probs[0])}
                }
            
            return scene_classification
            
        except Exception as e:
            print(f"âš ï¸ Scene ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            return self._basic_scene_classification(frame)
    
    def _basic_scene_classification(self, frame):
        """ê¸°ë³¸ Scene ë¶„ë¥˜ (íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜)"""
        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            brightness = np.mean(gray)
            
            if brightness > 120:
                time_label = 'day'
                time_confidence = 0.7
            else:
                time_label = 'night'
                time_confidence = 0.7
            
            edges = cv2.Canny(gray, 100, 200)
            edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
            
            if edge_density > 0.05:
                location_label = 'outdoor'
                location_confidence = 0.6
            else:
                location_label = 'indoor'
                location_confidence = 0.6
            
            return {
                'location': {
                    'label': location_label,
                    'confidence': location_confidence
                },
                'time': {
                    'label': time_label,
                    'confidence': time_confidence
                },
                'activity': {
                    'label': 'general',
                    'confidence': 0.5
                }
            }
            
        except Exception as e:
            print(f"âš ï¸ ê¸°ë³¸ Scene ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            return {}

class SceneGraphGenerator:
    """Scene Graph ìƒì„±ê¸° - ê°ì²´ê°„ ê´€ê³„ ë¶„ì„"""
    
    def __init__(self):
        if NETWORKX_AVAILABLE:
            self.graph = nx.DiGraph()
            self.use_networkx = True
        else:
            self.graph = None
            self.use_networkx = False
            print("âš ï¸ NetworkX ì—†ìŒ - ê¸°ë³¸ ê´€ê³„ ë¶„ì„ë§Œ ìˆ˜í–‰")
        
    def analyze_spatial_relationships(self, objects):
        """ê³µê°„ì  ê´€ê³„ ë¶„ì„"""
        relationships = []
        
        for i, obj1 in enumerate(objects):
            for j, obj2 in enumerate(objects[i+1:], i+1):
                bbox1 = obj1['bbox']
                bbox2 = obj2['bbox']
                
                center1 = [(bbox1[0] + bbox1[2])/2, (bbox1[1] + bbox1[3])/2]
                center2 = [(bbox2[0] + bbox2[2])/2, (bbox2[1] + bbox2[3])/2]
                
                distance = np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)
                
                if distance < 0.1:
                    relation = 'near'
                elif distance > 0.5:
                    relation = 'far_from'
                else:
                    if center1[1] < center2[1] - 0.05:
                        relation = 'above'
                    elif center1[1] > center2[1] + 0.05:
                        relation = 'under'
                    elif center1[0] < center2[0] - 0.05:
                        relation = 'left_of'
                    elif center1[0] > center2[0] + 0.05:
                        relation = 'right_of'
                    else:
                        relation = 'near'
                
                relationships.append({
                    'subject': obj1['class'],
                    'subject_id': obj1.get('track_id', i),
                    'predicate': relation,
                    'object': obj2['class'],
                    'object_id': obj2.get('track_id', j),
                    'confidence': min(obj1['confidence'], obj2['confidence']),
                    'distance': float(distance)
                })
        
        return relationships
    
    def analyze_semantic_relationships(self, objects, scene_context):
        """ì˜ë¯¸ì  ê´€ê³„ ë¶„ì„"""
        relationships = []
        
        semantic_rules = {
            'person': {
                'car': 'driving',
                'bicycle': 'riding',
                'chair': 'sitting_on',
                'laptop': 'using',
                'cell_phone': 'holding'
            },
            'car': {
                'traffic_light': 'stopped_at',
                'stop_sign': 'stopped_at'
            }
        }
        
        for obj1 in objects:
            for obj2 in objects:
                if obj1['class'] != obj2['class']:
                    if obj1['class'] in semantic_rules:
                        if obj2['class'] in semantic_rules[obj1['class']]:
                            relation = semantic_rules[obj1['class']][obj2['class']]
                            
                            relationships.append({
                                'subject': obj1['class'],
                                'subject_id': obj1.get('track_id', 0),
                                'predicate': relation,
                                'object': obj2['class'],
                                'object_id': obj2.get('track_id', 0),
                                'confidence': min(obj1['confidence'], obj2['confidence']),
                                'type': 'semantic'
                            })
        
        return relationships
    
    def build_scene_graph(self, objects, scene_context):
        """Scene Graph êµ¬ì¶•"""
        spatial_relations = self.analyze_spatial_relationships(objects)
        semantic_relations = self.analyze_semantic_relationships(objects, scene_context)
        
        all_relations = spatial_relations + semantic_relations
        
        if self.use_networkx:
            self.graph.clear()
            
            for obj in objects:
                node_id = f"{obj['class']}_{obj.get('track_id', 0)}"
                self.graph.add_node(node_id, **obj)
            
            for relation in all_relations:
                subject_id = f"{relation['subject']}_{relation['subject_id']}"
                object_id = f"{relation['object']}_{relation['object_id']}"
                
                if subject_id in self.graph.nodes and object_id in self.graph.nodes:
                    self.graph.add_edge(subject_id, object_id, **relation)
            
            scene_graph = {
                'nodes': dict(self.graph.nodes(data=True)),
                'edges': [
                    {
                        'source': edge[0],
                        'target': edge[1],
                        'relationship': edge[2]
                    }
                    for edge in self.graph.edges(data=True)
                ],
                'relationships': all_relations,
                'graph_metrics': {
                    'num_nodes': self.graph.number_of_nodes(),
                    'num_edges': self.graph.number_of_edges(),
                    'density': nx.density(self.graph) if self.graph.number_of_nodes() > 0 else 0
                }
            }
        else:
            scene_graph = {
                'nodes': {f"{obj['class']}_{obj.get('track_id', i)}": obj for i, obj in enumerate(objects)},
                'edges': [],
                'relationships': all_relations,
                'graph_metrics': {
                    'num_nodes': len(objects),
                    'num_edges': len(all_relations),
                    'density': 0
                }
            }
        
        return scene_graph

class GPTFeatureExtractor:
    """GPT ê¸°ë°˜ ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œê¸°"""
    
    def __init__(self):
        self.feature_dimensions = 1024
        
    def extract_gpt_features(self, frame_analysis, scene_context):
        """GPT ê¸°ë°˜ ê³ ìˆ˜ì¤€ íŠ¹ì§• ë²¡í„° ìƒì„±"""
        try:
            objects_info = scene_context.get('objects', [])
            scene_classification = scene_context.get('scene_classification', {})
            
            prompt = f"""
            ë‹¤ìŒ ë¹„ë””ì˜¤ í”„ë ˆì„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ ìˆ˜ì¤€ íŠ¹ì§•ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

            ê°ì²´ ì •ë³´: {[obj.get('class', '') for obj in objects_info]}
            ê°ì²´ ìˆ˜: {len(objects_info)}
            Scene ë¶„ë¥˜: {scene_classification}
            ê´€ê³„ ìˆ˜: {len(scene_context.get('relationships', []))}
            
            ë‹¤ìŒ 8ê°œ íŠ¹ì§•ì„ 0-1 ì‚¬ì´ ê°’ìœ¼ë¡œ ì •ëŸ‰í™”í•´ì£¼ì„¸ìš”:
            1. activity_type (í™œë™ ìœ í˜•)
            2. emotional_tone (ê°ì •ì  ë¶„ìœ„ê¸°)  
            3. complexity_level (ë³µì¡ì„± ìˆ˜ì¤€)
            4. interaction_level (ìƒí˜¸ì‘ìš© ì •ë„)
            5. visual_coherence (ì‹œê°ì  ì¼ê´€ì„±)
            6. spatial_composition (ê³µê°„ì  êµ¬ì„±)
            7. temporal_context (ì‹œê°„ì  ë§¥ë½)
            8. social_context (ì‚¬íšŒì  ìƒí™©)

            JSON í˜•íƒœë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
            {{"activity_type": 0.x, "emotional_tone": 0.x, ...}}
            """
            
            response = call_groq_llm_enhanced(
                prompt, 
                "ë¹„ë””ì˜¤ ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ì •í™•í•œ ìˆ˜ì¹˜ë¡œ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.",
                model="llama-3.1-8b-instant"
            )
            
            try:
                if '{' in response and '}' in response:
                    json_start = response.find('{')
                    json_end = response.rfind('}') + 1
                    json_str = response[json_start:json_end]
                    features = json.loads(json_str)
                    print(f"âœ… GPT íŠ¹ì§• ì¶”ì¶œ ì„±ê³µ: {len(features)}ê°œ íŠ¹ì§•")
                else:
                    raise ValueError("JSON í˜•ì‹ ì‘ë‹µ ì—†ìŒ")
            except:
                print("âš ï¸ GPT ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ íŠ¹ì§• ì‚¬ìš©")
                features = self._generate_default_features(scene_context)
            
            feature_vector = self._features_to_vector(features)
            
            return {
                'gpt_features': features,
                'feature_vector': feature_vector,
                'vector_dimension': len(feature_vector),
                'extraction_method': 'gpt_llm' if isinstance(features, dict) and len(features) > 4 else 'default'
            }
            
        except Exception as e:
            print(f"âš ï¸ GPT íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return self._generate_default_features(scene_context)
    
    def _generate_default_features(self, scene_context):
        """ê¸°ë³¸ íŠ¹ì§• ìƒì„±"""
        objects = scene_context.get('objects', [])
        relationships = scene_context.get('relationships', [])
        scene_class = scene_context.get('scene_classification', {})
        
        object_count = len(objects)
        relationship_count = len(relationships)
        
        features = {
            'activity_type': min(object_count * 0.15 + relationship_count * 0.1, 1.0),
            'emotional_tone': 0.6 if scene_class.get('time', {}).get('label') == 'day' else 0.4,
            'complexity_level': min(object_count * 0.12 + relationship_count * 0.08, 1.0),
            'interaction_level': min(relationship_count * 0.15, 1.0),
            'visual_coherence': 0.7 + min(object_count * 0.02, 0.2),
            'spatial_composition': 0.6 + min(relationship_count * 0.05, 0.3),
            'temporal_context': 0.5,
            'social_context': 1.0 if any(obj.get('class') == 'person' for obj in objects) else 0.2
        }
        
        feature_vector = list(features.values()) + [0.0] * (self.feature_dimensions - len(features))
        
        return {
            'gpt_features': features,
            'feature_vector': feature_vector[:self.feature_dimensions],
            'vector_dimension': self.feature_dimensions,
            'extraction_method': 'default_heuristic'
        }
    
    def _features_to_vector(self, features):
        """íŠ¹ì§• ë”•ì…”ë„ˆë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        if isinstance(features, dict):
            vector = []
            for key in sorted(features.keys()):
                value = features[key]
                if isinstance(value, (int, float)):
                    vector.append(float(value))
                else:
                    vector.append(0.0)
            
            while len(vector) < self.feature_dimensions:
                vector.append(0.0)
            
            return vector[:self.feature_dimensions]
        else:
            return [0.0] * self.feature_dimensions

class EnhancedCaptionGenerator:
    """ê°œì„ ëœ ìº¡ì…˜ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.temporal_context = []
        self.max_context_frames = 3
        
    def generate_accurate_caption(self, frame, detected_objects, scene_analysis, frame_id, timestamp):
        """ì •í™•ë„ê°€ í–¥ìƒëœ ìº¡ì…˜ ìƒì„±"""
        
        # 1. ê¸°ë³¸ ê°ì²´ ì •ë³´ ë¶„ì„
        object_info = self._analyze_objects_detailed(detected_objects)
        
        # 2. ê³µê°„ì  ê´€ê³„ ë¶„ì„
        spatial_info = self._analyze_spatial_relationships(detected_objects)
        
        # 3. ì‹œê°„ì  ë§¥ë½ ë¶„ì„
        temporal_info = self._analyze_temporal_context(detected_objects, frame_id)
        
        # 4. ì‹œê°ì  íŠ¹ì§• ë¶„ì„
        visual_info = self._analyze_visual_features(frame)
        
        # 5. Scene ë¶„ë¥˜ ì •ë³´
        scene_info = scene_analysis.get('scene_classification', {})
        
        # 6. VQA ê²°ê³¼ í™œìš©
        vqa_info = scene_analysis.get('vqa_results', {})
        
        # 7. OCR í…ìŠ¤íŠ¸ ì •ë³´
        ocr_text = scene_analysis.get('ocr_text', '')
        
        # 8. ì¢…í•© ìº¡ì…˜ ìƒì„±
        caption = self._generate_comprehensive_caption(
            object_info, spatial_info, temporal_info, visual_info, 
            scene_info, vqa_info, ocr_text, frame_id, timestamp
        )
        
        # 9. ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        self._update_temporal_context(detected_objects, frame_id)
        
        return caption
    
    def _analyze_objects_detailed(self, detected_objects):
        """ê°ì²´ë“¤ì˜ ìƒì„¸ ë¶„ì„"""
        if not detected_objects:
            return {"count": 0, "types": [], "colors": [], "positions": []}
        
        object_types = [obj['class'] for obj in detected_objects]
        object_counter = Counter(object_types)
        
        colors = []
        for obj in detected_objects:
            if obj.get('color_description') and obj['color_description'] != 'unknown':
                colors.append(f"{obj['color_description']} {obj['class']}")
        
        positions = []
        for obj in detected_objects:
            bbox = obj['bbox']
            center_x = (bbox[0] + bbox[2]) / 2
            center_y = (bbox[1] + bbox[3]) / 2
            
            if center_x < 0.33:
                x_pos = "ì™¼ìª½"
            elif center_x > 0.67:
                x_pos = "ì˜¤ë¥¸ìª½"
            else:
                x_pos = "ì¤‘ì•™"
                
            if center_y < 0.33:
                y_pos = "ìƒë‹¨"
            elif center_y > 0.67:
                y_pos = "í•˜ë‹¨"
            else:
                y_pos = "ì¤‘ê°„"
            
            positions.append(f"{x_pos} {y_pos}ì˜ {obj['class']}")
        
        return {
            "count": len(detected_objects),
            "types": object_counter,
            "colors": colors,
            "positions": positions,
            "main_objects": [item[0] for item in object_counter.most_common(3)]
        }
    
    def _analyze_spatial_relationships(self, detected_objects):
        """ê³µê°„ì  ê´€ê³„ ë¶„ì„"""
        if len(detected_objects) < 2:
            return []
        
        relationships = []
        for i, obj1 in enumerate(detected_objects):
            for obj2 in detected_objects[i+1:]:
                bbox1 = obj1['bbox']
                bbox2 = obj2['bbox']
                
                center1 = [(bbox1[0] + bbox1[2])/2, (bbox1[1] + bbox1[3])/2]
                center2 = [(bbox2[0] + bbox2[2])/2, (bbox2[1] + bbox2[3])/2]
                
                distance = np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)
                
                if distance < 0.15:
                    relationships.append(f"{obj1['class']}ì™€ {obj2['class']}ê°€ ê°€ê¹ê²Œ ìœ„ì¹˜")
                elif center1[0] < center2[0] - 0.1:
                    relationships.append(f"{obj1['class']}ê°€ {obj2['class']} ì™¼ìª½ì— ìœ„ì¹˜")
                elif center1[0] > center2[0] + 0.1:
                    relationships.append(f"{obj1['class']}ê°€ {obj2['class']} ì˜¤ë¥¸ìª½ì— ìœ„ì¹˜")
                elif center1[1] < center2[1] - 0.1:
                    relationships.append(f"{obj1['class']}ê°€ {obj2['class']} ìœ„ìª½ì— ìœ„ì¹˜")
                elif center1[1] > center2[1] + 0.1:
                    relationships.append(f"{obj1['class']}ê°€ {obj2['class']} ì•„ë˜ìª½ì— ìœ„ì¹˜")
        
        return relationships[:5]
    
    def _analyze_temporal_context(self, detected_objects, frame_id):
        """ì‹œê°„ì  ë§¥ë½ ë¶„ì„"""
        if not self.temporal_context:
            return {"is_first_frame": True, "changes": []}
        
        current_objects = set(obj['class'] for obj in detected_objects)
        
        if self.temporal_context:
            prev_objects = self.temporal_context[-1]['objects']
            
            new_objects = current_objects - prev_objects
            disappeared_objects = prev_objects - current_objects
            
            changes = []
            if new_objects:
                changes.append(f"ìƒˆë¡œ ë‚˜íƒ€ë‚œ ê°ì²´: {', '.join(new_objects)}")
            if disappeared_objects:
                changes.append(f"ì‚¬ë¼ì§„ ê°ì²´: {', '.join(disappeared_objects)}")
            
            return {
                "is_first_frame": False,
                "changes": changes,
                "continuity": len(current_objects & prev_objects) / max(len(current_objects | prev_objects), 1)
            }
        
        return {"is_first_frame": True, "changes": []}
    
    def _analyze_visual_features(self, frame):
        """ì‹œê°ì  íŠ¹ì§• ë¶„ì„"""
        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            brightness = np.mean(gray)
            
            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
            
            hist_h = cv2.calcHist([hsv], [0], None, [180], [0, 180])
            dominant_hue = np.argmax(hist_h)
            
            saturation = np.mean(hsv[:, :, 1])
            
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
            
            return {
                "brightness": "ë°ì€" if brightness > 120 else "ì–´ë‘ìš´",
                "saturation": "ì„ ëª…í•œ" if saturation > 100 else "íë¦°",
                "complexity": "ë³µì¡í•œ" if edge_density > 0.05 else "ë‹¨ìˆœí•œ",
                "dominant_color_hue": dominant_hue
            }
            
        except Exception as e:
            print(f"âš ï¸ ì‹œê°ì  íŠ¹ì§• ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"brightness": "ë³´í†µ", "saturation": "ë³´í†µ", "complexity": "ë³´í†µ"}
    
    def _generate_comprehensive_caption(self, object_info, spatial_info, temporal_info, 
                                      visual_info, scene_info, vqa_info, ocr_text, 
                                      frame_id, timestamp):
        """ì¢…í•©ì ì¸ ìº¡ì…˜ ìƒì„±"""
        
        caption_parts = []
        
        # 1. ì‹œê°„ ì •ë³´
        caption_parts.append(f"ì˜ìƒ {timestamp:.1f}ì´ˆ ì§€ì ì—ì„œ")
        
        # 2. ì¥ì†Œ/í™˜ê²½ ì •ë³´
        location = scene_info.get('location', {}).get('label', '')
        time_of_day = scene_info.get('time', {}).get('label', '')
        
        if location and time_of_day:
            caption_parts.append(f"{time_of_day} {location} í™˜ê²½ì—ì„œ")
        elif location:
            caption_parts.append(f"{location} í™˜ê²½ì—ì„œ")
        elif time_of_day:
            caption_parts.append(f"{time_of_day} ì‹œê°„ëŒ€ì—")
        
        # 3. ì£¼ìš” ê°ì²´ ë° ê°œìˆ˜
        if object_info['count'] > 0:
            main_objects = object_info['main_objects'][:3]
            
            if len(main_objects) == 1:
                if object_info['types'][main_objects[0]] > 1:
                    caption_parts.append(f"{object_info['types'][main_objects[0]]}ê°œì˜ {main_objects[0]}ì´/ê°€")
                else:
                    caption_parts.append(f"{main_objects[0]}ì´/ê°€")
            else:
                objects_desc = []
                for obj in main_objects:
                    count = object_info['types'][obj]
                    if count > 1:
                        objects_desc.append(f"{count}ê°œì˜ {obj}")
                    else:
                        objects_desc.append(obj)
                caption_parts.append(f"{', '.join(objects_desc)}ì´/ê°€")
        
        # 4. ìƒ‰ìƒ ì •ë³´ (ìˆëŠ” ê²½ìš°)
        if object_info['colors']:
            colors_text = ", ".join(object_info['colors'][:2])
            caption_parts.append(f"({colors_text})")
        
        # 5. ìœ„ì¹˜ ì •ë³´
        if object_info['positions']:
            positions_text = object_info['positions'][0]
            caption_parts.append(f"{positions_text}ì—ì„œ")
        
        # 6. í–‰ë™/í™œë™ ì¶”ì •
        action = self._estimate_action(object_info, spatial_info, vqa_info)
        if action:
            caption_parts.append(action)
        
        # 7. ê³µê°„ì  ê´€ê³„ (ì¤‘ìš”í•œ ê²ƒë§Œ)
        if spatial_info:
            relationship = spatial_info[0]
            caption_parts.append(f", {relationship}")
        
        # 8. ì‹œê°„ì  ë³€í™” (ìˆëŠ” ê²½ìš°)
        if not temporal_info.get('is_first_frame', True) and temporal_info.get('changes'):
            change = temporal_info['changes'][0]
            caption_parts.append(f". {change}")
        
        # 9. ì‹œê°ì  íŠ¹ì§•
        visual_desc = f"{visual_info['brightness']} {visual_info['complexity']} ì¥ë©´"
        caption_parts.append(f"ìœ¼ë¡œ {visual_desc}ì…ë‹ˆë‹¤")
        
        # 10. OCR í…ìŠ¤íŠ¸ (ìˆëŠ” ê²½ìš°)
        if ocr_text and len(ocr_text.strip()) > 0:
            caption_parts.append(f". í…ìŠ¤íŠ¸ '{ocr_text[:20]}...'ê°€ í™•ì¸ë©ë‹ˆë‹¤")
        
        # ë¬¸ì¥ ê²°í•© ë° ì •ë¦¬
        caption = " ".join(caption_parts)
        
        # ë¬¸ì¥ ì •ë¦¬
        caption = self._clean_caption(caption)
        
        return caption
    
    def _estimate_action(self, object_info, spatial_info, vqa_info):
        """ê°ì²´ì™€ ê´€ê³„ë¥¼ ë°”íƒ•ìœ¼ë¡œ í–‰ë™/í™œë™ ì¶”ì •"""
        
        activity_keywords = {
            "walking": "ê±·ê³  ìˆëŠ”",
            "sitting": "ì•‰ì•„ ìˆëŠ”",
            "standing": "ì„œ ìˆëŠ”",
            "driving": "ìš´ì „í•˜ê³  ìˆëŠ”",
            "eating": "ë¨¹ê³  ìˆëŠ”",
            "drinking": "ë§ˆì‹œê³  ìˆëŠ”",
            "reading": "ì½ê³  ìˆëŠ”",
            "working": "ì‘ì—…í•˜ê³  ìˆëŠ”",
            "playing": "ë†€ê³  ìˆëŠ”",
            "talking": "ëŒ€í™”í•˜ê³  ìˆëŠ”"
        }
        
        # VQA ê²°ê³¼ì—ì„œ í™œë™ ì¶”ì •
        for question, answer in vqa_info.items():
            if "í™œë™" in question or "activity" in question.lower():
                for keyword, korean in activity_keywords.items():
                    if keyword in answer.lower():
                        return korean
        
        # ê°ì²´ ê¸°ë°˜ í™œë™ ì¶”ì •
        main_objects = object_info.get('main_objects', [])
        
        if 'person' in main_objects:
            if 'car' in main_objects:
                return "ì´ë™í•˜ê³  ìˆëŠ”"
            elif 'laptop' in main_objects or 'computer' in main_objects:
                return "ì‘ì—…í•˜ê³  ìˆëŠ”"
            elif 'chair' in main_objects:
                return "ì•‰ì•„ ìˆëŠ”"
            elif any(food in main_objects for food in ['apple', 'banana', 'sandwich', 'pizza']):
                return "ì‹ì‚¬í•˜ê³  ìˆëŠ”"
            else:
                return "í™œë™í•˜ê³  ìˆëŠ”"
        elif 'car' in main_objects and 'traffic_light' in main_objects:
            return "êµí†µ ìƒí™©ì—ì„œ ì •ì§€í•´ ìˆëŠ”"
        elif len(main_objects) > 3:
            return "ë‹¤ì–‘í•œ í™œë™ì´ ì¼ì–´ë‚˜ê³  ìˆëŠ”"
        else:
            return "ìœ„ì¹˜í•´ ìˆëŠ”"
    
    def _clean_caption(self, caption):
        """ìº¡ì…˜ ì •ë¦¬ ë° ìµœì í™”"""
        
        if len(caption) > 500:
            caption = caption[:500] + "..."
        
        words = caption.split()
        cleaned_words = []
        prev_word = ""
        
        for word in words:
            if word != prev_word:
                cleaned_words.append(word)
            prev_word = word
        
        caption = " ".join(cleaned_words)
        
        caption = caption.replace("ì´/ê°€ ì´/ê°€", "ì´/ê°€")
        caption = caption.replace("ì—ì„œ ì—ì„œ", "ì—ì„œ")
        caption = caption.replace("  ", " ")
        
        return caption.strip()
    
    def _update_temporal_context(self, detected_objects, frame_id):
        """ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸"""
        current_objects = set(obj['class'] for obj in detected_objects)
        
        self.temporal_context.append({
            'frame_id': frame_id,
            'objects': current_objects,
            'count': len(detected_objects)
        })
        
        if len(self.temporal_context) > self.max_context_frames:
            self.temporal_context.pop(0)

class AdvancedSceneAnalyzer:
    """ê³ ê¸‰ Scene ë¶„ì„ê¸°"""
    
    def __init__(self, enable_vqa=True, enable_ocr=True, enable_segmentation=True):
        self.enable_vqa = enable_vqa
        self.enable_ocr = enable_ocr
        self.enable_segmentation = enable_segmentation
        
        # ìº¡ì…˜ ìƒì„±ê¸° ì´ˆê¸°í™”
        self.caption_generator = EnhancedCaptionGenerator()
        
        # OCR ëª¨ë¸ ì´ˆê¸°í™”
        if enable_ocr and OCR_AVAILABLE:
            try:
                self.ocr_reader = easyocr.Reader(['en', 'ko'])
                # print("ğŸ”¤ OCR ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except:
                self.ocr_reader = None
                print("âš ï¸ OCR ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        else:
            self.ocr_reader = None
        
        # VQA ëª¨ë¸ ì´ˆê¸°í™”
        if enable_vqa and TRANSFORMERS_AVAILABLE:
            try:
                # print("ğŸ¤– VQA ëª¨ë¸ ë¡œë”© ì¤‘...")
                self.vqa_processor = BlipProcessor.from_pretrained("Salesforce/blip-vqa-base")
                self.vqa_model = BlipForQuestionAnswering.from_pretrained(
                    "Salesforce/blip-vqa-base",
                    torch_dtype=torch.float32
                )
                
                self.caption_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
                self.caption_model = BlipForConditionalGeneration.from_pretrained(
                    "Salesforce/blip-image-captioning-base",
                    torch_dtype=torch.float32
                )
                
                print("ğŸ¤– VQA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                
            except Exception as e:
                self.vqa_processor = None
                self.vqa_model = None
                self.caption_processor = None
                self.caption_model = None
                print(f"âš ï¸ VQA ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            self.vqa_processor = None
            self.vqa_model = None
            self.caption_processor = None
            self.caption_model = None
        
        # MediaPipe í¬ì¦ˆ ì¶”ì •
        if MEDIAPIPE_AVAILABLE:
            try:
                self.mp_pose = mp.solutions.pose
                self.pose = self.mp_pose.Pose()
                print("ğŸƒ MediaPipe í¬ì¦ˆ ì¶”ì • ë¡œë“œ ì™„ë£Œ")
            except:
                self.mp_pose = None
                self.pose = None
                print("âš ï¸ MediaPipe ë¡œë“œ ì‹¤íŒ¨")
        else:
            self.mp_pose = None
            self.pose = None
        
        # ì¶”ê°€ êµ¬ì„± ìš”ì†Œ
        self.scene_classifier = SceneClassifier()
        self.scene_graph_generator = SceneGraphGenerator()
        self.gpt_extractor = GPTFeatureExtractor()
    
    def comprehensive_scene_analysis(self, frame, frame_id, timestamp, detected_objects):
        """ì¢…í•©ì ì¸ Scene ë¶„ì„"""
        analysis_result = {
            'frame_id': frame_id,
            'timestamp': timestamp,
            'basic_info': {},
            'scene_classification': {},
            'scene_graph': {},
            'gpt_features': {},
            'ocr_text': "",
            'vqa_results': {},
            'visual_features': [],
            'pose_features': []
        }
        
        try:
            # 1. ê¸°ë³¸ ì •ë³´
            analysis_result['basic_info'] = {
                'frame_shape': frame.shape,
                'detected_objects_count': len(detected_objects),
                'object_types': list(set(obj.get('class', '') for obj in detected_objects))
            }
            
            # 2. Scene ë¶„ë¥˜
            analysis_result['scene_classification'] = self.scene_classifier.classify_scene(frame)
            
            # 3. OCR
            if self.enable_ocr and self.ocr_reader:
                analysis_result['ocr_text'] = self.extract_scene_text(frame)
            
            # 4. VQA
            if self.enable_vqa:
                vqa_questions = [
                    "What is happening in this image?",
                    "What is the main activity?",
                    "How many people are in the image?",
                    "What is the setting or location?",
                    "What time of day is it?",
                    "What objects are visible?"
                ]
                analysis_result['vqa_results'] = self.answer_vqa_questions(frame, vqa_questions)
            
            # 5. Scene Graph ìƒì„±
            scene_context = {
                'objects': detected_objects,
                'scene_classification': analysis_result['scene_classification'],
                'vqa_results': analysis_result['vqa_results']
            }
            analysis_result['scene_graph'] = self.scene_graph_generator.build_scene_graph(
                detected_objects, scene_context
            )
            
            # 6. GPT ê¸°ë°˜ ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ
            analysis_result['gpt_features'] = self.gpt_extractor.extract_gpt_features(
                analysis_result, scene_context
            )
            
            # 7. ì‹œê°ì  íŠ¹ì§•
            analysis_result['visual_features'] = self.extract_visual_features(frame).tolist()
            
            # 8. í¬ì¦ˆ íŠ¹ì§•
            analysis_result['pose_features'] = self.extract_pose_features(frame)
            
        except Exception as e:
            print(f"âš ï¸ ì¢…í•© ë¶„ì„ ì˜¤ë¥˜: {e}")
        
        return analysis_result
    
    def generate_enhanced_caption(self, frame, detected_objects, scene_analysis, frame_id, timestamp):
        """ê°œì„ ëœ ìº¡ì…˜ ìƒì„±"""
        
        # 1. BLIP ê¸°ë³¸ ìº¡ì…˜ ìƒì„±
        blip_caption = self.generate_advanced_caption(frame)
        
        # 2. í–¥ìƒëœ ìº¡ì…˜ ìƒì„±ê¸° ì‚¬ìš©
        enhanced_caption = self.caption_generator.generate_accurate_caption(
            frame, detected_objects, scene_analysis, frame_id, timestamp
        )
        
        # 3. LLM ê¸°ë°˜ ìµœì¢… ìº¡ì…˜ ìƒì„±
        final_caption = self._generate_llm_caption(
            frame, detected_objects, scene_analysis, blip_caption, enhanced_caption, frame_id, timestamp
        )
        
        return {
            'blip_caption': blip_caption,
            'enhanced_caption': enhanced_caption,
            'final_caption': final_caption,
            'caption_length': len(final_caption),
            'has_multiple_sources': True
        }
    
    def _generate_llm_caption(self, frame, detected_objects, scene_analysis, blip_caption, enhanced_caption, frame_id, timestamp):
        """LLM ê¸°ë°˜ ìµœì¢… ìº¡ì…˜ ìƒì„±"""
        
        try:
            scene_info = scene_analysis.get('scene_classification', {})
            vqa_info = scene_analysis.get('vqa_results', {})
            relationships = scene_analysis.get('scene_graph', {}).get('relationships', [])
            ocr_text = scene_analysis.get('ocr_text', '')
            
            object_details = []
            for obj in detected_objects:
                detail = f"{obj['class']}"
                if obj.get('color_description') and obj['color_description'] != 'unknown':
                    detail += f" ({obj['color_description']})"
                if obj.get('confidence'):
                    detail += f" [ì‹ ë¢°ë„: {obj['confidence']:.2f}]"
                object_details.append(detail)
            
            relationship_summary = []
            for rel in relationships[:3]:
                relationship_summary.append(f"{rel['subject']} {rel['predicate']} {rel['object']}")
            
            key_vqa_info = {}
            for question, answer in vqa_info.items():
                if "happening" in question.lower() or "activity" in question.lower():
                    key_vqa_info['main_activity'] = answer
                elif "people" in question.lower():
                    key_vqa_info['people_count'] = answer
                elif "setting" in question.lower() or "location" in question.lower():
                    key_vqa_info['location'] = answer
                elif "time" in question.lower():
                    key_vqa_info['time'] = answer
            
            llm_prompt = f"""
            ë‹¤ìŒì€ ë¹„ë””ì˜¤ í”„ë ˆì„ {frame_id} ({timestamp:.1f}ì´ˆ)ì˜ ì¢…í•© ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. 
            ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ í•œêµ­ì–´ ìº¡ì…˜ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

            == ê°ì²´ ê°ì§€ ê²°ê³¼ ==
            ì´ {len(detected_objects)}ê°œ ê°ì²´ ê°ì§€:
            {', '.join(object_details)}

            == Scene ë¶„ë¥˜ ==
            - ì¥ì†Œ: {scene_info.get('location', {}).get('label', 'ë¶ˆëª…')} (ì‹ ë¢°ë„: {scene_info.get('location', {}).get('confidence', 0):.2f})
            - ì‹œê°„ëŒ€: {scene_info.get('time', {}).get('label', 'ë¶ˆëª…')} (ì‹ ë¢°ë„: {scene_info.get('time', {}).get('confidence', 0):.2f})
            - í™œë™: {scene_info.get('activity', {}).get('label', 'ë¶ˆëª…')}

            == ê°ì²´ê°„ ê´€ê³„ ==
            {'; '.join(relationship_summary) if relationship_summary else 'íŠ¹ë³„í•œ ê´€ê³„ ì—†ìŒ'}

            == VQA ë¶„ì„ ê²°ê³¼ ==
            {json.dumps(key_vqa_info, ensure_ascii=False, indent=2) if key_vqa_info else 'ë¶„ì„ ë°ì´í„° ì—†ìŒ'}

            == ê¸°ì¡´ ìº¡ì…˜ë“¤ ==
            BLIP ìë™ ìº¡ì…˜: {blip_caption}
            í–¥ìƒëœ ìº¡ì…˜: {enhanced_caption}

            == OCR í…ìŠ¤íŠ¸ ==
            {ocr_text if ocr_text else 'í…ìŠ¤íŠ¸ ì—†ìŒ'}

            ìš”ì²­ì‚¬í•­:
            1. ìœ„ì˜ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ìº¡ì…˜ì„ ì‘ì„±í•´ì£¼ì„¸ìš”
            2. ê°ì²´ì˜ ìœ„ì¹˜, ìƒ‰ìƒ, ê°œìˆ˜ë¥¼ ì •í™•íˆ ë°˜ì˜í•´ì£¼ì„¸ìš”
            3. ê°ì²´ê°„ì˜ ê´€ê³„ì™€ ìƒí˜¸ì‘ìš©ì„ í¬í•¨í•´ì£¼ì„¸ìš”
            4. Sceneì˜ ì‹œê°„ëŒ€, ì¥ì†Œ, ë¶„ìœ„ê¸°ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”
            5. 150-300ì ê¸¸ì´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”
            6. ê¸°ì¡´ ìº¡ì…˜ë“¤ì˜ ì¢‹ì€ ë¶€ë¶„ì€ í™œìš©í•˜ë˜, ë” ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ê°œì„ í•´ì£¼ì„¸ìš”

            ìº¡ì…˜:
            """
            
            final_caption = call_groq_llm_enhanced(
                llm_prompt,
                "ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ëª¨ë“  ì •ë³´ë¥¼ ì •í™•íˆ ë°˜ì˜í•˜ì—¬ ìƒìƒí•˜ê³  ìƒì„¸í•œ í•œêµ­ì–´ ìº¡ì…˜ì„ ì‘ì„±í•©ë‹ˆë‹¤.",
                model="llama-3.1-8b-instant"
            )
            
            if final_caption and len(final_caption) > 50:
                if "ìº¡ì…˜:" in final_caption:
                    final_caption = final_caption.split("ìº¡ì…˜:")[-1].strip()
                
                print(f"âœ… LLM ìµœì¢… ìº¡ì…˜ ìƒì„± ì™„ë£Œ ({len(final_caption)}ì)")
                return final_caption
            else:
                print("âš ï¸ LLM ìº¡ì…˜ í’ˆì§ˆ ë¶€ì¡±, í–¥ìƒëœ ìº¡ì…˜ ì‚¬ìš©")
                return enhanced_caption
                
        except Exception as e:
            print(f"âš ï¸ LLM ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return enhanced_caption if enhanced_caption else blip_caption
    
    def generate_advanced_caption(self, frame):
        """BLIP ëª¨ë¸ë¡œ ê³ ê¸‰ ìº¡ì…˜ ìƒì„±"""
        if not self.caption_processor or not self.caption_model:
            return ""
        
        try:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(rgb_frame)
            
            inputs = self.caption_processor(image, return_tensors="pt")
            
            out = self.caption_model.generate(**inputs, max_length=50, do_sample=False)
            caption = self.caption_processor.decode(out[0], skip_special_tokens=True)
            return caption
        except Exception as e:
            print(f"âš ï¸ BLIP ìº¡ì…˜ ìƒì„± ì˜¤ë¥˜: {e}")
            return ""
    
    def extract_scene_text(self, frame):
        """OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if not self.ocr_reader:
            return ""
        
        try:
            results = self.ocr_reader.readtext(frame)
            text_list = [result[1] for result in results if result[2] > 0.5]
            return " ".join(text_list)
        except Exception as e:
            print(f"âš ï¸ OCR ì˜¤ë¥˜: {e}")
            return ""
    
    def answer_vqa_questions(self, frame, questions):
        """VQAë¡œ íŠ¹ì • ì§ˆë¬¸ì— ë‹µë³€"""
        if not self.vqa_processor or not self.vqa_model:
            return {}
        
        answers = {}
        try:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(rgb_frame)
            
            for question in questions:
                inputs = self.vqa_processor(image, question, return_tensors="pt")
                out = self.vqa_model.generate(**inputs, max_length=50)
                answer = self.vqa_processor.decode(out[0], skip_special_tokens=True)
                answers[question] = answer
                
        except Exception as e:
            print(f"âš ï¸ VQA ì˜¤ë¥˜: {e}")
        
        return answers
    
    def extract_pose_features(self, frame):
        """MediaPipeë¡œ í¬ì¦ˆ íŠ¹ì§• ì¶”ì¶œ"""
        if not self.pose:
            return []
        
        try:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.pose.process(rgb_frame)
            
            if results.pose_landmarks:
                landmarks = []
                for landmark in results.pose_landmarks.landmark:
                    landmarks.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])
                return landmarks
            else:
                return []
        except Exception as e:
            print(f"âš ï¸ í¬ì¦ˆ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def extract_visual_features(self, frame):
        """ì‹œê°ì  íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
        try:
            # 1. ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨
            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
            hist_h = cv2.calcHist([hsv], [0], None, [50], [0, 180])
            hist_s = cv2.calcHist([hsv], [1], None, [50], [0, 256])
            hist_v = cv2.calcHist([hsv], [2], None, [50], [0, 256])
            
            # 2. ì—£ì§€ íŠ¹ì§•
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray, 100, 200)
            edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
            
            # 3. í…ìŠ¤ì²˜ íŠ¹ì§•
            texture_score = np.std(gray)
            
            # 4. ë°ê¸° íŠ¹ì§•
            brightness = np.mean(gray)
            contrast = np.std(gray)
            
            # íŠ¹ì§• ë²¡í„° ê²°í•©
            features = np.concatenate([
                hist_h.flatten(),
                hist_s.flatten(), 
                hist_v.flatten(),
                [edge_density, texture_score, brightness, contrast]
            ])
            
            return features / np.linalg.norm(features)  # ì •ê·œí™”
            
        except Exception as e:
            print(f"âš ï¸ ì‹œê°ì  íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return np.zeros(154)
# ê¸°ì¡´ ì½”ë“œë¥¼ ë‹¤ìŒìœ¼ë¡œ êµì²´

# video_analyzer.py - EnhancedVideoAnalyzer í´ë˜ìŠ¤ __init__ ë° _load_yolo_model ìˆ˜ì •

class EnhancedVideoAnalyzer:
    """ìµœê³ ê¸‰ ë¹„ë””ì˜¤ ë¶„ì„ê¸° - Django í†µí•©"""
    
    def __init__(self, model_path="yolov8x.pt", confidence_threshold=0.25, **kwargs):
        
        self.model = None
        self.confidence_threshold = confidence_threshold
        
        # ì„¤ì •ê°’ë“¤
        self.enable_color_analysis = kwargs.get('enable_color_analysis', True)
        self.enable_tracking = kwargs.get('enable_tracking', True)
        self.enable_scene_analysis = kwargs.get('enable_scene_analysis', True)
        self.enable_scene_classification = kwargs.get('enable_scene_classification', True)
        self.enable_segmentation = kwargs.get('enable_segmentation', True)
        self.enable_scene_graph = kwargs.get('enable_scene_graph', True)
        self.enable_vqa = kwargs.get('enable_vqa', True)
        self.enable_ocr = kwargs.get('enable_ocr', True)
        self.enable_gpt_features = kwargs.get('enable_gpt_features', True)
        
        # ì‹œìŠ¤í…œ ê¸°ëŠ¥ ìƒíƒœ ì†ì„± ì¶”ê°€
        self.clip_available = TRANSFORMERS_AVAILABLE
        self.ocr_available = OCR_AVAILABLE
        self.vqa_available = TRANSFORMERS_AVAILABLE
        self.scene_graph_available = NETWORKX_AVAILABLE
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸ§  Enhanced ë¹„ë””ì˜¤ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘... (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # YOLO ëª¨ë¸ ë¡œë”© (ê°œì„ ëœ ì˜ˆì™¸ì²˜ë¦¬)
        if YOLO_AVAILABLE:
            try:
                self._load_yolo_model(model_path)
            except Exception as e:
                log_once(f"âš ï¸ YOLO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ë¶„ì„ ê¸°ëŠ¥ ì œí•œ): {e}", "WARNING")
                self.model = None
        else:
            log_once("â„¹ï¸ YOLO ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ - ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰", "INFO")
        
        # ë¶„ì„ ëª¨ë“ˆ ì´ˆê¸°í™” (ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”)
        self._init_analysis_modules()
        
        log_once(f"âœ… Enhanced ë¹„ë””ì˜¤ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ", "SUCCESS")
        log_once(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥: YOLO={self.model is not None}, CLIP={self.clip_available}, OCR={self.ocr_available}, VQA={self.vqa_available}", "INFO")
    
    def _load_yolo_model(self, model_path):
        """YOLO ëª¨ë¸ ë¡œë”© - ê°œì„ ëœ ë²„ì „"""
        try:
            log_once(f"ğŸ”„ YOLO ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}", "INFO")
            
            # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ í™•ì¸
            if not os.path.exists(model_path):
                # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œë„
                log_once(f"ğŸ“¥ ì‚¬ì „ í›ˆë ¨ëœ YOLO ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_path}", "INFO")
            
            self.model = YOLO(model_path)
            
            # ëª¨ë¸ì„ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if hasattr(self.model, 'to'):
                self.model = self.model.to(self.device)
            
            log_once(f"âœ… YOLO ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path} (ë””ë°”ì´ìŠ¤: {self.device})", "SUCCESS")
            
            # ëª¨ë¸ ì •ë³´ ë¡œê·¸
            if hasattr(self.model, 'names'):
                class_count = len(self.model.names)
                log_once(f"ğŸ“‹ ê°ì§€ ê°€ëŠ¥í•œ í´ë˜ìŠ¤ ìˆ˜: {class_count}ê°œ", "INFO")
            
        except Exception as e:
            log_once(f"âŒ YOLO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}", "ERROR")
            self.model = None
            
            # êµ¬ì²´ì ì¸ í•´ê²° ë°©ì•ˆ ì œì‹œ
            if "No module named 'ultralytics'" in str(e):
                log_once("ğŸ’¡ í•´ê²° ë°©ë²•: pip install ultralytics", "INFO")
            elif "CUDA" in str(e):
                log_once("ğŸ’¡ CUDA ì˜¤ë¥˜ - CPU ëª¨ë“œë¡œ ì „í™˜ ì‹œë„", "INFO")
                try:
                    self.device = "cpu"
                    self.model = YOLO(model_path)
                    log_once("âœ… CPU ëª¨ë“œë¡œ YOLO ëª¨ë¸ ë¡œë“œ ì„±ê³µ", "SUCCESS")
                except:
                    self.model = None
            elif "download" in str(e).lower():
                log_once("ğŸ’¡ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - ì¸í„°ë„· ì—°ê²° í™•ì¸ í•„ìš”", "INFO")
    
    def _init_analysis_modules(self):
        """ë¶„ì„ ëª¨ë“ˆ ì´ˆê¸°í™”"""
        try:
            if self.enable_color_analysis:
                self.color_analyzer = ColorAnalyzer()
                log_once("ğŸ¨ ìƒ‰ìƒ ë¶„ì„ê¸° í™œì„±í™”", "INFO")
        except Exception as e:
            log_once(f"âš ï¸ ìƒ‰ìƒ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", "WARNING")
        
        try:
            if self.enable_scene_analysis:
                self.scene_analyzer = AdvancedSceneAnalyzer(
                    enable_vqa=self.enable_vqa,
                    enable_ocr=self.enable_ocr,
                    enable_segmentation=self.enable_segmentation
                )
                log_once("ğŸ¬ ê³ ê¸‰ Scene ë¶„ì„ê¸° í™œì„±í™”", "INFO")
                
                # ì†ì„± ì—…ë°ì´íŠ¸
                if hasattr(self.scene_analyzer, 'ocr_reader') and self.scene_analyzer.ocr_reader:
                    self.ocr_available = True
                if hasattr(self.scene_analyzer, 'vqa_model') and self.scene_analyzer.vqa_model:
                    self.vqa_available = True
                    
        except Exception as e:
            log_once(f"âš ï¸ Scene ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", "WARNING")
            self.scene_analyzer = None
    
    def detect_objects_comprehensive(self, frame):
        """ì¢…í•©ì ì¸ ê°ì²´ ê°ì§€ - ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”"""
        if self.model is None:
            # ì²« ë²ˆì§¸ ê²½ê³ ë§Œ ì¶œë ¥í•˜ë„ë¡ ê°œì„ 
            if not hasattr(self, '_no_yolo_warning_shown'):
                log_once("âš ï¸ YOLO ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ - ê¸°ë³¸ ë¶„ì„ ìˆ˜í–‰", "WARNING")
                self._no_yolo_warning_shown = True
            return []
            
        try:
            if self.enable_tracking:
                results = self.model.track(frame, verbose=False, persist=True, conf=self.confidence_threshold)
            else:
                results = self.model(frame, verbose=False, conf=self.confidence_threshold)
            
            detected_objects = []
            
            for result in results:
                if result.boxes is not None and len(result.boxes) > 0:
                    boxes = result.boxes.xyxy.cpu().numpy()
                    confidences = result.boxes.conf.cpu().numpy()
                    class_ids = result.boxes.cls.cpu().numpy().astype(int)
                    
                    if self.enable_tracking and result.boxes.id is not None:
                        track_ids = result.boxes.id.cpu().numpy().astype(int)
                    else:
                        track_ids = np.arange(len(boxes))
                    
                    h, w = frame.shape[:2]
                    
                    for i, (box, conf, cls_id, track_id) in enumerate(zip(boxes, confidences, class_ids, track_ids)):
                        if conf > self.confidence_threshold and cls_id in ENHANCED_OBJECTS:
                            class_name = ENHANCED_OBJECTS[cls_id]
                            
                            normalized_box = [
                                round(float(box[0]) / w, 4),
                                round(float(box[1]) / h, 4),
                                round(float(box[2]) / w, 4),
                                round(float(box[3]) / h, 4)
                            ]
                            
                            obj_info = {
                                'class': class_name,
                                'bbox': normalized_box,
                                'track_id': int(track_id),
                                'confidence': float(conf),
                                'colors': [],
                                'color_description': "unknown"
                            }
                            
                            # ìƒ‰ìƒ ë¶„ì„ (ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€)
                            if self.enable_color_analysis and hasattr(self, 'color_analyzer'):
                                try:
                                    colors = self.color_analyzer.extract_dominant_colors(frame, normalized_box)
                                    obj_info['colors'] = colors
                                    obj_info['color_description'] = self.color_analyzer.get_color_description(colors)
                                except Exception as e:
                                    # ìƒ‰ìƒ ë¶„ì„ ì‹¤íŒ¨ëŠ” ë¡œê·¸ ì—†ì´ ê¸°ë³¸ê°’ ì‚¬ìš©
                                    pass
                            
                            detected_objects.append(obj_info)
            
            return detected_objects
            
        except Exception as e:
            log_once(f"âŒ ê°ì²´ ê°ì§€ ì˜¤ë¥˜: {e}", "ERROR")
            return []
    
    def analyze_video_comprehensive(self, video, analysis_type='comprehensive', progress_callback=None):
        """ë¹„ë””ì˜¤ ì¢…í•© ë¶„ì„ - Django ëª¨ë¸ê³¼ ì—°ë™"""
        try:
            from django.conf import settings
            import os
            
            # ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì°¾ê¸° (ê°œì„ ë¨)
            video_path = self._find_video_path(video)
            if not video_path:
                raise Exception("ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise Exception("ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            duration = total_frames / fps if fps > 0 else 0
            
            # ë¹„ë””ì˜¤ ì •ë³´ ì—…ë°ì´íŠ¸
            video.width = width
            video.height = height
            video.fps = fps
            video.total_frames = total_frames
            video.duration = duration
            video.save()
            
            # í”„ë ˆì„ ìƒ˜í”Œë§ (ë¶„ì„ íƒ€ì…ì— ë”°ë¼ ì¡°ì •)
            sample_intervals = {
                'basic': max(1, total_frames // 30),
                'enhanced': max(1, total_frames // 50),
                'comprehensive': max(1, total_frames // 100),
                'custom': max(1, total_frames // 75)
            }
            sample_interval = sample_intervals.get(analysis_type, max(1, total_frames // 50))
            
            frame_results = []
            processed_frames = 0
            frame_id = 0
            
            log_once(f"ğŸ¬ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘: {video.original_name} ({analysis_type})", "INFO")
            log_once(f"ğŸ“Š ì´ í”„ë ˆì„: {total_frames}, ìƒ˜í”Œë§ ê°„ê²©: {sample_interval}", "INFO")
            
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_id += 1
                
                # ìƒ˜í”Œë§ ì²´í¬
                if frame_id % sample_interval != 1:
                    continue
                
                timestamp = frame_id / fps
                
                try:
                    # 1. ê°ì²´ ê°ì§€
                    detected_objects = self.detect_objects_comprehensive(frame)
                    
                    # 2. Scene ë¶„ì„
                    scene_analysis = {}
                    if self.enable_scene_analysis and hasattr(self, 'scene_analyzer') and self.scene_analyzer:
                        scene_analysis = self.scene_analyzer.comprehensive_scene_analysis(
                            frame, frame_id, timestamp, detected_objects
                        )
                    
                    # 3. í–¥ìƒëœ ìº¡ì…˜ ìƒì„±
                    caption_result = {}
                    if self.enable_scene_analysis and hasattr(self, 'scene_analyzer') and self.scene_analyzer:
                        caption_result = self.scene_analyzer.generate_enhanced_caption(
                            frame, detected_objects, scene_analysis, frame_id, timestamp
                        )
                    else:
                        # ê¸°ë³¸ ìº¡ì…˜
                        basic_caption = f"í”„ë ˆì„ {frame_id} ({timestamp:.1f}ì´ˆ)ì—ì„œ {len(detected_objects)}ê°œì˜ ê°ì²´ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
                        if detected_objects:
                            object_list = [obj['class'] for obj in detected_objects[:3]]
                            basic_caption += f" ì£¼ìš” ê°ì²´: {', '.join(object_list)}"
                        
                        caption_result = {
                            'blip_caption': "",
                            'enhanced_caption': basic_caption,
                            'final_caption': basic_caption,
                            'caption_length': len(basic_caption),
                            'has_multiple_sources': False
                        }
                    
                    # 4. í”„ë ˆì„ ë°ì´í„° êµ¬ì„±
                    frame_dict = {
                        "image_id": frame_id,
                        "timestamp": timestamp,
                        "objects": detected_objects,
                        "scene_analysis": scene_analysis,
                        "caption": caption_result.get('final_caption', ''),
                        "blip_caption": caption_result.get('blip_caption', ''),
                        "enhanced_caption": caption_result.get('enhanced_caption', ''),
                        "final_caption": caption_result.get('final_caption', ''),
                        "caption_sources": caption_result,
                        "comprehensive_features": {
                            "object_count": len(detected_objects),
                            "object_types": list(set(obj['class'] for obj in detected_objects)),
                            "scene_complexity": len(scene_analysis.get('scene_graph', {}).get('relationships', [])),
                            "has_text": bool(scene_analysis.get('ocr_text', '')),
                            "has_caption": bool(caption_result.get('final_caption', '')),
                            "caption_length": caption_result.get('caption_length', 0),
                            "caption_quality": "enhanced" if caption_result.get('has_multiple_sources', False) else "basic",
                            "analysis_success": True
                        }
                    }
                    
                    frame_results.append(frame_dict)
                    processed_frames += 1
                    
                    # ì§„í–‰ë¥  ì½œë°± (ë¡œê·¸ ì¤‘ë³µ ë°©ì§€)
                    if progress_callback and processed_frames % 5 == 0:  # 5í”„ë ˆì„ë§ˆë‹¤ë§Œ ì½œë°±
                        progress = (frame_id / total_frames) * 100
                        progress_callback(progress, f"í”„ë ˆì„ {frame_id}/{total_frames} ë¶„ì„ ì¤‘")
                
                except Exception as e:
                    log_once(f"âŒ í”„ë ˆì„ {frame_id} ë¶„ì„ ì‹¤íŒ¨: {e}", "ERROR")
                    continue
            
            cap.release()
            
            # ê²°ê³¼ ìš”ì•½
            # video_summary = self._summarize_video_analysis(frame_results, video)
            # video_analyzer.py - analyze_video_comprehensive ë©”ì„œë“œì—ì„œ ë‹¤ìŒ ë¶€ë¶„ë§Œ ìˆ˜ì •

# # âŒ í˜„ì¬ ì˜¤ë¥˜ê°€ ë‚˜ëŠ” ì½”ë“œ (ì´ ì¤„ì„ ì°¾ìœ¼ì„¸ìš”)
# video_summary = self._summarize_video_analysis(frame_results, video)

            # âœ… ë‹¤ìŒ ì½”ë“œë¡œ êµì²´ (ì•ˆì „í•œ ì„ì‹œ ìˆ˜ì •)
            try:
                if hasattr(self, '_summarize_video_analysis'):
                    video_summary = self._summarize_video_analysis(frame_results, video)
                else:
                    # ì„ì‹œ ìš”ì•½ ìƒì„±
                    log_once("âš ï¸ _summarize_video_analysis ë©”ì„œë“œ ì—†ìŒ, ê¸°ë³¸ ìš”ì•½ ìƒì„±", "WARNING")
                    video_summary = self._create_basic_summary(frame_results, video)
            except Exception as e:
                log_once(f"âš ï¸ ë¹„ë””ì˜¤ ìš”ì•½ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ìš”ì•½ ì‚¬ìš©: {e}", "WARNING")
                video_summary = self._create_basic_summary(frame_results, video)

            # ê·¸ë¦¬ê³  EnhancedVideoAnalyzer í´ë˜ìŠ¤ì— ë‹¤ìŒ ë©”ì„œë“œë„ ì¶”ê°€:
            def _create_basic_summary(self, frame_results, video):
                """ê¸°ë³¸ ìš”ì•½ ìƒì„± (fallback)"""
                from collections import Counter
                
                all_objects = []
                for frame_result in frame_results:
                    if 'objects' in frame_result:
                        for obj in frame_result['objects']:
                            if 'class' in obj:
                                all_objects.append(obj['class'])
                
                object_counter = Counter(all_objects)
                dominant_objects = [obj for obj, count in object_counter.most_common(10)]
                
                return {
                    'total_frames': len(frame_results),
                    'analysis_features': ['object_detection', 'basic_analysis'],
                    'dominant_objects': dominant_objects,
                    'text_content': '',
                    'scene_types': ['general'],
                    'highlight_frames': []
                }
            log_once(f"âœ… ë¹„ë””ì˜¤ ë¶„ì„ ì™„ë£Œ: {processed_frames}ê°œ í”„ë ˆì„ ì²˜ë¦¬", "SUCCESS")
            
            return {
                'video_summary': video_summary,
                'frame_results': frame_results,
                'analysis_config': {
                    'method': 'Enhanced_MultiModal_Analysis',
                    'analysis_type': analysis_type,
                    'sample_interval': sample_interval,
                    'features_enabled': {
                        'yolo': self.model is not None,
                        'clip': self.clip_available,
                        'ocr': self.ocr_available,
                        'vqa': self.vqa_available,
                        'scene_graph': self.scene_graph_available
                    }
                },
                'total_frames_analyzed': processed_frames,
                'success': True
            }
            
        except Exception as e:
            log_once(f"âŒ ë¹„ë””ì˜¤ ë¶„ì„ ì‹¤íŒ¨: {e}", "ERROR")
            return {'error': str(e), 'success': False}
    # video_analyzer.py - EnhancedVideoAnalyzer í´ë˜ìŠ¤ì— ì¶”ê°€í•  ë©”ì„œë“œ

    def _summarize_video_analysis(self, frame_results, video):
        """ë¹„ë””ì˜¤ ë¶„ì„ ê²°ê³¼ ìš”ì•½ - ëˆ„ë½ëœ ë©”ì„œë“œ ì¶”ê°€"""
        try:
            log_once("ğŸ“Š ë¹„ë””ì˜¤ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„± ì¤‘...", "INFO")
            
            summary = {
                'total_frames': len(frame_results),
                'analysis_features': [],
                'dominant_objects': [],
                'text_content': '',
                'scene_types': [],
                'highlight_frames': [],
                'analysis_quality_metrics': {},
                'processing_statistics': {}
            }
            
            if not frame_results:
                log_once("âš ï¸ ë¶„ì„í•  í”„ë ˆì„ ê²°ê³¼ê°€ ì—†ìŒ", "WARNING")
                return summary
            
            # ê° í”„ë ˆì„ ê²°ê³¼ í†µí•©
            all_objects = []
            all_texts = []
            scene_types = []
            caption_lengths = []
            confidence_scores = []
            
            for frame_result in frame_results:
                # ê°ì²´ ìˆ˜ì§‘
                if 'comprehensive_features' in frame_result:
                    object_types = frame_result['comprehensive_features'].get('object_types', [])
                    all_objects.extend(object_types)
                
                # ì¼ë°˜ ê°ì²´ ì •ë³´ë„ ìˆ˜ì§‘
                if 'objects' in frame_result:
                    for obj in frame_result['objects']:
                        if 'class' in obj:
                            all_objects.append(obj['class'])
                        if 'confidence' in obj:
                            confidence_scores.append(obj['confidence'])
                
                # í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (OCR ê²°ê³¼)
                scene_analysis = frame_result.get('scene_analysis', {})
                ocr_text = scene_analysis.get('ocr_text', '')
                if ocr_text and isinstance(ocr_text, str) and len(ocr_text.strip()) > 0:
                    all_texts.append(ocr_text.strip())
                
                # ìº¡ì…˜ ê¸¸ì´ ìˆ˜ì§‘
                final_caption = frame_result.get('final_caption', '')
                enhanced_caption = frame_result.get('enhanced_caption', '')
                caption = final_caption or enhanced_caption or frame_result.get('caption', '')
                if caption:
                    caption_lengths.append(len(caption))
                
                # ì”¬ íƒ€ì… ìˆ˜ì§‘
                if scene_analysis:
                    scene_classification = scene_analysis.get('scene_classification', {})
                    location = scene_classification.get('location', {}).get('label', '')
                    time_label = scene_classification.get('time', {}).get('label', '')
                    activity = scene_classification.get('activity', {}).get('label', '')
                    
                    for scene_type in [location, time_label, activity]:
                        if scene_type and scene_type != 'unknown':
                            scene_types.append(scene_type)
            
            # ìš”ì•½ ì •ë³´ ìƒì„±
            if all_objects:
                from collections import Counter
                object_counter = Counter(all_objects)
                summary['dominant_objects'] = [obj for obj, count in object_counter.most_common(15)]
                
                # ë¶„ì„ í’ˆì§ˆ ë©”íŠ¸ë¦­
                summary['analysis_quality_metrics']['unique_object_types'] = len(object_counter)
                summary['analysis_quality_metrics']['total_object_detections'] = len(all_objects)
                summary['analysis_quality_metrics']['average_objects_per_frame'] = len(all_objects) / len(frame_results)
            
            if all_texts:
                # í…ìŠ¤íŠ¸ ë‚´ìš© ê²°í•© (ìµœëŒ€ 1000ì)
                combined_text = ' '.join(all_texts)[:1000]
                summary['text_content'] = combined_text
                summary['analysis_quality_metrics']['text_extraction_frames'] = len(all_texts)
                summary['analysis_quality_metrics']['total_text_length'] = len(combined_text)
            
            if scene_types:
                scene_counter = Counter(scene_types)
                summary['scene_types'] = [scene for scene, count in scene_counter.most_common(8)]
                summary['analysis_quality_metrics']['scene_variety'] = len(scene_counter)
            
            if caption_lengths:
                summary['analysis_quality_metrics']['average_caption_length'] = sum(caption_lengths) / len(caption_lengths)
                summary['analysis_quality_metrics']['frames_with_captions'] = len(caption_lengths)
            
            if confidence_scores:
                summary['analysis_quality_metrics']['average_detection_confidence'] = sum(confidence_scores) / len(confidence_scores)
                summary['analysis_quality_metrics']['high_confidence_detections'] = len([c for c in confidence_scores if c > 0.8])
            
            # ì²˜ë¦¬ í†µê³„
            summary['processing_statistics'] = {
                'frames_processed': len(frame_results),
                'frames_with_objects': len([f for f in frame_results if f.get('objects', [])]),
                'frames_with_scene_analysis': len([f for f in frame_results if f.get('scene_analysis')]),
                'frames_with_enhanced_captions': len([f for f in frame_results if f.get('enhanced_caption')]),
                'processing_success_rate': (len(frame_results) / len(frame_results)) * 100 if frame_results else 0
            }
            
            # í•˜ì´ë¼ì´íŠ¸ í”„ë ˆì„ ì„ ì • (ê°ì²´ê°€ ë§ì´ ê°ì§€ëœ í”„ë ˆì„ë“¤)
            highlight_candidates = []
            for i, frame_result in enumerate(frame_results):
                object_count = len(frame_result.get('objects', []))
                scene_complexity = frame_result.get('comprehensive_features', {}).get('scene_complexity', 0)
                caption_quality = 1 if frame_result.get('enhanced_caption') else 0
                
                score = object_count * 2 + scene_complexity + caption_quality
                highlight_candidates.append({
                    'frame_index': i,
                    'frame_id': frame_result.get('image_id', i),
                    'timestamp': frame_result.get('timestamp', 0),
                    'score': score,
                    'object_count': object_count
                })
            
            # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 5ê°œ ì„ ì •
            highlight_candidates.sort(key=lambda x: x['score'], reverse=True)
            summary['highlight_frames'] = highlight_candidates[:5]
            
            # ì‚¬ìš©ëœ ë¶„ì„ ê¸°ëŠ¥ë“¤
            analysis_features = set()
            for frame_result in frame_results:
                if frame_result.get('objects'):
                    analysis_features.add('object_detection')
                if frame_result.get('scene_analysis', {}).get('scene_classification'):
                    analysis_features.add('scene_classification')
                if frame_result.get('scene_analysis', {}).get('ocr_text'):
                    analysis_features.add('ocr')
                if frame_result.get('scene_analysis', {}).get('vqa_results'):
                    analysis_features.add('vqa')
                if frame_result.get('enhanced_caption'):
                    analysis_features.add('enhanced_caption')
                if frame_result.get('scene_analysis', {}).get('gpt_features'):
                    analysis_features.add('gpt_features')
            
            summary['analysis_features'] = list(analysis_features)
            summary['analysis_quality_metrics']['features_used_count'] = len(analysis_features)
            
            log_once(f"âœ… ë¹„ë””ì˜¤ ë¶„ì„ ìš”ì•½ ì™„ë£Œ: {len(frame_results)}í”„ë ˆì„, {len(summary['dominant_objects'])}ê°œ ê°ì²´ ìœ í˜•", "SUCCESS")
            
            return summary
            
        except Exception as e:
            log_once(f"âŒ ë¹„ë””ì˜¤ ë¶„ì„ ìš”ì•½ ì‹¤íŒ¨: {e}", "ERROR")
            
            # ê¸°ë³¸ ìš”ì•½ ë°˜í™˜
            return {
                'total_frames': len(frame_results) if frame_results else 0,
                'analysis_features': ['basic_analysis'],
                'dominant_objects': [],
                'text_content': '',
                'scene_types': [],
                'highlight_frames': [],
                'analysis_quality_metrics': {'error': str(e)},
                'processing_statistics': {'status': 'error', 'frames_processed': len(frame_results) if frame_results else 0}
            }
        
    def search_comprehensive(self, video, query):
        """ì¢…í•©ì ì¸ ë¹„ë””ì˜¤ ê²€ìƒ‰"""
        try:
            # Frameì—ì„œ ê²€ìƒ‰
            frames = Frame.objects.filter(video=video)
            search_results = []
            
            for frame in frames:
                # ìº¡ì…˜ì—ì„œ ê²€ìƒ‰
                if query.lower() in frame.final_caption.lower():
                    search_results.append({
                        'frame_id': frame.image_id,
                        'timestamp': frame.timestamp,
                        'caption': frame.final_caption,
                        'match_type': 'caption',
                        'confidence': 0.8
                    })
                
                # ê°ì²´ì—ì„œ ê²€ìƒ‰
                for obj in frame.detected_objects:
                    if query.lower() in obj.get('class', '').lower():
                        search_results.append({
                            'frame_id': frame.image_id,
                            'timestamp': frame.timestamp,
                            'object_class': obj.get('class'),
                            'match_type': 'object',
                            'confidence': obj.get('confidence', 0.5)
                        })
            
            return search_results[:20]  # ìƒìœ„ 20ê°œ ê²°ê³¼
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
        
    def _find_video_path(self, video):
        """ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì°¾ê¸° - ê°œì„ ë¨"""
        from django.conf import settings
        
        possible_paths = [
            os.path.join(settings.MEDIA_ROOT, 'videos', video.filename),
            os.path.join(settings.MEDIA_ROOT, 'uploads', video.filename),
            video.file_path if hasattr(video, 'file_path') else None
        ]
        
        # None ì œê±°
        possible_paths = [p for p in possible_paths if p]
        
        for path in possible_paths:
            if os.path.exists(path):
                return path
        
        return None
    
# video_analyzer.py íŒŒì¼ ë§¨ ë§ˆì§€ë§‰ ë¶€ë¶„ì— ì¶”ê°€/ìˆ˜ì •

# ì „ì—­ VideoAnalyzer ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_global_video_analyzer = None
_analyzer_lock = threading.Lock()  # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½

def get_video_analyzer():
    """ì „ì—­ VideoAnalyzer ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´) - ìŠ¤ë ˆë“œ ì•ˆì „"""
    global _global_video_analyzer
    
    # ì´ë¯¸ ì¸ìŠ¤í„´ìŠ¤ê°€ ìˆìœ¼ë©´ ë°˜í™˜
    if _global_video_analyzer is not None:
        return _global_video_analyzer
    
    # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½ ì‚¬ìš©
    with _analyzer_lock:
        # ë”ë¸” ì²´í¬ - ë‹¤ë¥¸ ìŠ¤ë ˆë“œê°€ ì´ë¯¸ ìƒì„±í–ˆì„ ìˆ˜ ìˆìŒ
        if _global_video_analyzer is not None:
            return _global_video_analyzer
        
        try:
            log_once("ğŸš€ ì „ì—­ VideoAnalyzer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘...", "INFO")
            
            # EnhancedVideoAnalyzer ìš°ì„  ìƒì„± ì‹œë„
            try:
                _global_video_analyzer = EnhancedVideoAnalyzer()
                log_once("âœ… EnhancedVideoAnalyzer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ", "SUCCESS")
            except Exception as enhanced_error:
                log_once(f"âš ï¸ EnhancedVideoAnalyzer ìƒì„± ì‹¤íŒ¨: {enhanced_error}", "WARNING")
                
                # Fallback: ê¸°ë³¸ VideoAnalyzer ìƒì„± ì‹œë„
                try:
                    if 'VideoAnalyzer' in globals():
                        _global_video_analyzer = VideoAnalyzer()
                        log_once("âœ… ê¸°ë³¸ VideoAnalyzer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ", "SUCCESS")
                    else:
                        raise ImportError("VideoAnalyzer í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                except Exception as basic_error:
                    log_once(f"âŒ ê¸°ë³¸ VideoAnalyzer ìƒì„±ë„ ì‹¤íŒ¨: {basic_error}", "ERROR")
                    
                    # ìµœì¢… Fallback: ìµœì†Œ ê¸°ëŠ¥ ë”ë¯¸ í´ë˜ìŠ¤
                    _global_video_analyzer = _create_dummy_analyzer()
                    log_once("âš ï¸ ë”ë¯¸ VideoAnalyzer ìƒì„±ë¨ (ì œí•œëœ ê¸°ëŠ¥)", "WARNING")
            
            return _global_video_analyzer
            
        except Exception as e:
            log_once(f"âŒ VideoAnalyzer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ì „ ì‹¤íŒ¨: {e}", "ERROR")
            
            # ìµœì¢… Fallback
            _global_video_analyzer = _create_dummy_analyzer()
            return _global_video_analyzer

def _create_dummy_analyzer():
    """ìµœì†Œ ê¸°ëŠ¥ ë”ë¯¸ VideoAnalyzer í´ë˜ìŠ¤"""
    class DummyVideoAnalyzer:
        def __init__(self):
            self.model = None
            self.clip_available = False
            self.ocr_available = False
            self.vqa_available = False
            self.scene_graph_available = False
            self.device = "cpu"
            log_once("âš ï¸ ë”ë¯¸ VideoAnalyzer ì´ˆê¸°í™”ë¨", "WARNING")
        
        def detect_objects_comprehensive(self, frame):
            """ë”ë¯¸ ê°ì²´ ê°ì§€"""
            return []
        
        def analyze_video_comprehensive(self, video, analysis_type='basic', progress_callback=None):
            """ë”ë¯¸ ë¹„ë””ì˜¤ ë¶„ì„"""
            if progress_callback:
                progress_callback(50, "ë”ë¯¸ ë¶„ì„ ì§„í–‰ ì¤‘")
                progress_callback(100, "ë”ë¯¸ ë¶„ì„ ì™„ë£Œ")
            
            return {
                'success': True,
                'video_summary': {
                    'dominant_objects': [],
                    'scene_types': ['unknown'],
                    'text_content': ''
                },
                'frame_results': [],
                'total_frames_analyzed': 0,
                'analysis_config': {
                    'method': 'Dummy_Analysis',
                    'analysis_type': analysis_type,
                    'features_enabled': {
                        'yolo': False,
                        'clip': False,
                        'ocr': False,
                        'vqa': False,
                        'scene_graph': False
                    }
                }
            }
    
    return DummyVideoAnalyzer()

def reset_video_analyzer():
    """ì „ì—­ VideoAnalyzer ì¸ìŠ¤í„´ìŠ¤ ë¦¬ì…‹ (í…ŒìŠ¤íŠ¸/ê°œë°œìš©)"""
    global _global_video_analyzer
    with _analyzer_lock:
        _global_video_analyzer = None
        log_once("ğŸ”„ VideoAnalyzer ì¸ìŠ¤í„´ìŠ¤ ë¦¬ì…‹ë¨", "INFO")

def get_analyzer_status():
    """í˜„ì¬ VideoAnalyzer ìƒíƒœ ë°˜í™˜"""
    analyzer = get_video_analyzer()
    if analyzer is None:
        return {'status': 'not_initialized'}
    
    return {
        'status': 'initialized',
        'type': type(analyzer).__name__,
        'features': {
            'yolo': getattr(analyzer, 'model', None) is not None,
            'clip': getattr(analyzer, 'clip_available', False),
            'ocr': getattr(analyzer, 'ocr_available', False),
            'vqa': getattr(analyzer, 'vqa_available', False),
            'scene_graph': getattr(analyzer, 'scene_graph_available', False)
        },
        'device': getattr(analyzer, 'device', 'unknown')
    }

# ê¸°ì¡´ VideoAnalyzer í´ë˜ìŠ¤ë¥¼ í™•ì¥ (í˜¸í™˜ì„± ìœ ì§€)
class VideoAnalyzer(EnhancedVideoAnalyzer):
    """ê¸°ì¡´ ê¸°ëŠ¥ê³¼ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ í†µí•©í•œ VideoAnalyzer - í˜¸í™˜ì„± ìœ ì§€"""
    
    def __init__(self):
        try:
            super().__init__()
            log_once("âœ… VideoAnalyzer (í˜¸í™˜ì„± ë²„ì „) ì´ˆê¸°í™” ì™„ë£Œ", "SUCCESS")
        except Exception as e:
            log_once(f"âš ï¸ VideoAnalyzer ì´ˆê¸°í™” ë¶€ë¶„ ì‹¤íŒ¨: {e}", "WARNING")
            # ìµœì†Œí•œì˜ ì†ì„±ë“¤ì€ ì„¤ì •
            self.model = None
            self.clip_available = False
            self.ocr_available = False
            self.vqa_available = False
            self.scene_graph_available = False
            self.device = "cpu"

# ëª¨ë“ˆ ë¡œë“œ ì‹œ ì¦‰ì‹œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì„ íƒì‚¬í•­)
try:
    # import ì‹œì ì—ì„œ ë°”ë¡œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±í•˜ì§€ ì•ŠìŒ (ì§€ì—° ë¡œë”©)
    # get_video_analyzer()  # ì´ ì¤„ì„ ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ì§€ì—° ë¡œë”©
    log_once("ğŸ“¦ video_analyzer ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ", "INFO")
except Exception as module_error:
    log_once(f"âš ï¸ video_analyzer ëª¨ë“ˆ ë¡œë“œ ì¤‘ ê²½ê³ : {module_error}", "WARNING")