# integrated_analysis_pipeline.py - ê¸°ì¡´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì— ê³ ê¸‰ ê¸°ëŠ¥ í†µí•©

import os
import cv2
import json
import time
import threading
from datetime import datetime
from collections import defaultdict, Counter

from django.conf import settings
from .models import (
    Video, VideoAnalysis, Frame, Scene,
    VideoMetadata, PersonDetection, TemporalStatistics, 
    ObjectTracking, VideoSearchIndex
)
from .video_analyzer import get_video_analyzer
from .advanced_video_analyzer import (
    AdvancedVideoMetadataAnalyzer, PersonAttributeAnalyzer, 
    TemporalAnalyzer, VideoSearchEngine
)
from .llm_client import LLMClient


class IntegratedAnalysisPipeline:
    """í†µí•© ë¶„ì„ íŒŒì´í”„ë¼ì¸ - ê¸°ì¡´ + ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥"""
    
    def __init__(self):
        self.video_analyzer = get_video_analyzer()
        self.metadata_analyzer = AdvancedVideoMetadataAnalyzer()
        self.person_analyzer = PersonAttributeAnalyzer()
        self.temporal_analyzer = TemporalAnalyzer()
        self.llm_client = LLMClient()
        
        # ë¶„ì„ ìƒíƒœ ì¶”ì 
        self.analysis_progress = {}
        self.analysis_lock = threading.Lock()
        
        print("ğŸš€ í†µí•© ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def run_comprehensive_analysis(self, video, analysis_type='comprehensive', progress_callback=None):
        """ì¢…í•© ë¶„ì„ ì‹¤í–‰ - ê¸°ì¡´ + ê³ ê¸‰ ë¶„ì„ í†µí•©"""
        
        analysis_id = f"analysis_{video.id}_{int(time.time())}"
        
        try:
            print(f"ğŸ¬ ë¹„ë””ì˜¤ {video.id} ì¢…í•© ë¶„ì„ ì‹œì‘: {analysis_type}")
            
            # 1ë‹¨ê³„: ê¸°ì¡´ ê¸°ë³¸ ë¶„ì„ ì‹¤í–‰
            if progress_callback:
                progress_callback(5, "ê¸°ë³¸ AI ë¶„ì„ ì‹œì‘")
            
            basic_results = self._run_basic_analysis(video, progress_callback)
            
            # 2ë‹¨ê³„: ê³ ê¸‰ ë©”íƒ€ë°ì´í„° ë¶„ì„
            if progress_callback:
                progress_callback(25, "ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ë¶„ì„")
            
            metadata_results = self._analyze_video_metadata(video, basic_results)
            
            # 3ë‹¨ê³„: ì‚¬ëŒ ì†ì„± ë¶„ì„
            if progress_callback:
                progress_callback(45, "ì‚¬ëŒ ì†ì„± ë¶„ì„")
            
            person_results = self._analyze_person_attributes(video, basic_results)
            
            # 4ë‹¨ê³„: ì‹œê°„ë³„ í†µê³„ ìƒì„±
            if progress_callback:
                progress_callback(65, "ì‹œê°„ë³„ í†µê³„ ë¶„ì„")
            
            temporal_results = self._generate_temporal_statistics(video, person_results)
            
            # 5ë‹¨ê³„: ê°ì²´ ì¶”ì  ë¶„ì„
            if progress_callback:
                progress_callback(80, "ê°ì²´ ì¶”ì  ë¶„ì„")
            
            tracking_results = self._analyze_object_tracking(video, basic_results)
            
            # 6ë‹¨ê³„: ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±
            if progress_callback:
                progress_callback(90, "ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±")
            
            self._build_search_index(video, {
                'basic': basic_results,
                'metadata': metadata_results,
                'person': person_results,
                'temporal': temporal_results,
                'tracking': tracking_results
            })
            
            # 7ë‹¨ê³„: ê²°ê³¼ í†µí•© ë° ì €ì¥
            if progress_callback:
                progress_callback(95, "ê²°ê³¼ ì €ì¥")
            
            final_results = self._integrate_and_save_results(
                video, analysis_type, {
                    'basic': basic_results,
                    'metadata': metadata_results,
                    'person': person_results,
                    'temporal': temporal_results,
                    'tracking': tracking_results
                }
            )
            
            if progress_callback:
                progress_callback(100, "ë¶„ì„ ì™„ë£Œ")
            
            print(f"âœ… ë¹„ë””ì˜¤ {video.id} ì¢…í•© ë¶„ì„ ì™„ë£Œ")
            return final_results
            
        except Exception as e:
            print(f"âŒ ë¹„ë””ì˜¤ {video.id} ì¢…í•© ë¶„ì„ ì‹¤íŒ¨: {e}")
            import traceback
            print(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            # ì‹¤íŒ¨ ì‹œ ìƒíƒœ ì •ë¦¬
            video.analysis_status = 'failed'
            video.save()
            
            raise e
    
    def _run_basic_analysis(self, video, progress_callback):
        """ê¸°ì¡´ ê¸°ë³¸ ë¶„ì„ ì‹¤í–‰"""
        try:
            # ê¸°ì¡´ EnhancedVideoAnalyzerì˜ analyze_video_comprehensive ì‚¬ìš©
            if hasattr(self.video_analyzer, 'analyze_video_comprehensive'):
                
                def basic_progress_callback(progress, step):
                    # ê¸°ë³¸ ë¶„ì„ì€ ì „ì²´ì˜ 20% ì°¨ì§€
                    adjusted_progress = 5 + (progress * 0.2)  # 5% ~ 25%
                    if progress_callback:
                        progress_callback(adjusted_progress, f"ê¸°ë³¸ ë¶„ì„: {step}")
                
                basic_results = self.video_analyzer.analyze_video_comprehensive(
                    video, 
                    analysis_type='comprehensive',
                    progress_callback=basic_progress_callback
                )
                
                return basic_results
            else:
                # Fallback: ê°„ë‹¨í•œ í”„ë ˆì„ ë¶„ì„
                return self._fallback_basic_analysis(video, progress_callback)
                
        except Exception as e:
            print(f"âš ï¸ ê¸°ë³¸ ë¶„ì„ ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
            return self._fallback_basic_analysis(video, progress_callback)
    
    def _fallback_basic_analysis(self, video, progress_callback):
        """ê¸°ë³¸ ë¶„ì„ fallback"""
        # ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
        video_path = self._get_video_path(video)
        if not video_path:
            raise Exception("ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ê°„ë‹¨í•œ í”„ë ˆì„ ìƒ˜í”Œë§ ë° ë¶„ì„
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        frame_results = []
        sample_interval = max(1, total_frames // 50)  # ìµœëŒ€ 50ê°œ í”„ë ˆì„
        
        frame_count = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            if frame_count % sample_interval != 1:
                continue
            
            timestamp = frame_count / fps
            
            # ê¸°ë³¸ ê°ì²´ ê°ì§€
            detected_objects = []
            if hasattr(self.video_analyzer, 'detect_objects_comprehensive'):
                detected_objects = self.video_analyzer.detect_objects_comprehensive(frame)
            
            frame_data = {
                'image_id': frame_count,
                'timestamp': timestamp,
                'objects': detected_objects,
                'scene_analysis': {}
            }
            
            frame_results.append(frame_data)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress = (frame_count / total_frames) * 100
            if progress_callback and frame_count % 10 == 0:
                progress_callback(5 + (progress * 0.2), f"í”„ë ˆì„ {frame_count}/{total_frames} ë¶„ì„")
        
        cap.release()
        
        return {
            'success': True,
            'frame_results': frame_results,
            'video_summary': {
                'dominant_objects': self._extract_dominant_objects(frame_results),
                'scene_types': ['general'],
                'text_content': ''
            },
            'total_frames_analyzed': len(frame_results)
        }
    
    def _analyze_video_metadata(self, video, basic_results):
        """ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ë¶„ì„"""
        try:
            print(f"ğŸï¸ ë¹„ë””ì˜¤ {video.id} ë©”íƒ€ë°ì´í„° ë¶„ì„ ì‹œì‘")
            
            # í”„ë ˆì„ ìƒ˜í”Œë§ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ë¶„ì„
            video_path = self._get_video_path(video)
            if not video_path:
                raise Exception("ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            cap = cv2.VideoCapture(video_path)
            frame_analyses = []
            
            # 10ê°œ í”„ë ˆì„ ìƒ˜í”Œë§
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            sample_indices = [i * (total_frames // 10) for i in range(10)]
            
            for frame_idx in sample_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if not ret:
                    continue
                
                # ë‚ ì”¨ ë° ì‹œê°„ëŒ€ ë¶„ì„
                weather_analysis = self.metadata_analyzer.analyze_weather_conditions(frame)
                time_analysis = self.metadata_analyzer.analyze_time_of_day(frame)
                
                frame_analyses.append({
                    'weather': weather_analysis,
                    'time': time_analysis
                })
            
            cap.release()
            
            # ë©”íƒ€ë°ì´í„° ì§‘ê³„
            weather_votes = Counter()
            time_votes = Counter()
            brightness_values = []
            contrast_values = []
            
            for analysis in frame_analyses:
                if analysis['weather']['confidence'] > 0.3:
                    weather_votes[analysis['weather']['weather']] += analysis['weather']['confidence']
                
                if analysis['time']['confidence'] > 0.3:
                    time_votes[analysis['time']['time_period']] += analysis['time']['confidence']
                
                brightness_values.append(analysis['weather']['brightness'])
                contrast_values.append(analysis['weather']['contrast'])
            
            # ìµœì¢… ë©”íƒ€ë°ì´í„° ê²°ì •
            dominant_weather = weather_votes.most_common(1)[0][0] if weather_votes else 'unknown'
            weather_confidence = weather_votes[dominant_weather] / len(frame_analyses) if weather_votes else 0.0
            
            dominant_time = time_votes.most_common(1)[0][0] if time_votes else 'unknown'
            time_confidence = time_votes[dominant_time] / len(frame_analyses) if time_votes else 0.0
            
            # ì¥ì†Œ ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            dominant_objects = basic_results.get('video_summary', {}).get('dominant_objects', [])
            outdoor_objects = ['car', 'bicycle', 'motorcycle', 'traffic_light', 'stop_sign']
            indoor_objects = ['chair', 'laptop', 'tv', 'couch', 'bed']
            
            outdoor_score = sum(1 for obj in dominant_objects if obj in outdoor_objects)
            indoor_score = sum(1 for obj in dominant_objects if obj in indoor_objects)
            
            if outdoor_score > indoor_score:
                location_type = 'outdoor'
                location_confidence = 0.7
            elif indoor_score > outdoor_score:
                location_type = 'indoor'
                location_confidence = 0.7
            else:
                location_type = 'unknown'
                location_confidence = 0.3
            
            metadata_result = {
                'dominant_weather': dominant_weather,
                'weather_confidence': weather_confidence,
                'dominant_time_period': dominant_time,
                'time_confidence': time_confidence,
                'location_type': location_type,
                'location_confidence': location_confidence,
                'overall_brightness': sum(brightness_values) / len(brightness_values) if brightness_values else 128.0,
                'overall_contrast': sum(contrast_values) / len(contrast_values) if contrast_values else 50.0,
                'dominant_colors': [],
                'scene_complexity': len(dominant_objects) * 0.1
            }
            
            # VideoMetadata ëª¨ë¸ì— ì €ì¥
            VideoMetadata.objects.update_or_create(
                video=video,
                defaults=metadata_result
            )
            
            print(f"âœ… ë¹„ë””ì˜¤ {video.id} ë©”íƒ€ë°ì´í„° ë¶„ì„ ì™„ë£Œ: {dominant_weather}, {dominant_time}")
            return metadata_result
            
        except Exception as e:
            print(f"âš ï¸ ë©”íƒ€ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'dominant_weather': 'unknown',
                'weather_confidence': 0.0,
                'dominant_time_period': 'unknown',
                'time_confidence': 0.0,
                'location_type': 'unknown',
                'location_confidence': 0.0,
                'overall_brightness': 128.0,
                'overall_contrast': 50.0
            }
    
    def _analyze_person_attributes(self, video, basic_results):
        """ì‚¬ëŒ ì†ì„± ë¶„ì„"""
        try:
            print(f"ğŸ‘¤ ë¹„ë””ì˜¤ {video.id} ì‚¬ëŒ ì†ì„± ë¶„ì„ ì‹œì‘")
            
            frame_results = basic_results.get('frame_results', [])
            person_detections = []
            
            # ë¹„ë””ì˜¤ í”„ë ˆì„ ì¬ë¶„ì„ (ì‚¬ëŒ ì†ì„± ì¶”ì¶œì„ ìœ„í•´)
            video_path = self._get_video_path(video)
            if not video_path:
                raise Exception("ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            cap = cv2.VideoCapture(video_path)
            
            for frame_data in frame_results:
                frame_id = frame_data.get('image_id', 0)
                timestamp = frame_data.get('timestamp', 0)
                objects = frame_data.get('objects', [])
                
                # í•´ë‹¹ í”„ë ˆì„ìœ¼ë¡œ ì´ë™
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id - 1)
                ret, frame = cap.read()
                if not ret:
                    continue
                
                # ì‚¬ëŒ ê°ì²´ë“¤ì— ëŒ€í•œ ì†ì„± ë¶„ì„
                for person_idx, obj in enumerate(objects):
                    if obj.get('class') != 'person':
                        continue
                    
                    person_bbox = obj.get('bbox', [])
                    if len(person_bbox) != 4:
                        continue
                    
                    # ì‚¬ëŒ ì†ì„± ë¶„ì„
                    attributes = self.person_analyzer.analyze_person_attributes(frame, person_bbox)
                    
                    # PersonDetection ëª¨ë¸ì— ì €ì¥
                    try:
                        # Frame ëª¨ë¸ì—ì„œ í•´ë‹¹ í”„ë ˆì„ ì°¾ê¸°
                        frame_obj = Frame.objects.filter(
                            video=video, 
                            image_id=frame_id
                        ).first()
                        
                        if frame_obj:
                            PersonDetection.objects.create(
                                frame=frame_obj,
                                person_id=person_idx,
                                track_id=obj.get('track_id'),
                                bbox_x1=person_bbox[0],
                                bbox_y1=person_bbox[1],
                                bbox_x2=person_bbox[2],
                                bbox_y2=person_bbox[3],
                                confidence=obj.get('confidence', 0.5),
                                gender_estimation=attributes['gender_estimation']['gender'],
                                gender_confidence=attributes['gender_estimation']['confidence'],
                                upper_body_color=attributes['upper_body_color']['color'],
                                upper_color_confidence=attributes['upper_body_color']['confidence'],
                                lower_body_color=attributes['lower_body_color']['color'],
                                lower_color_confidence=attributes['lower_body_color']['confidence'],
                                posture=attributes['posture']['posture'],
                                posture_confidence=attributes['posture']['confidence'],
                                detailed_attributes=attributes
                            )
                    except Exception as save_error:
                        print(f"âš ï¸ PersonDetection ì €ì¥ ì‹¤íŒ¨: {save_error}")
                    
                    person_detections.append({
                        'frame_id': frame_id,
                        'timestamp': timestamp,
                        'person_id': person_idx,
                        'attributes': attributes,
                        'bbox': person_bbox
                    })
            
            cap.release()
            
            # ì‚¬ëŒ ì†ì„± í†µê³„ ìƒì„±
            person_stats = self._generate_person_statistics(person_detections)
            
            print(f"âœ… ë¹„ë””ì˜¤ {video.id} ì‚¬ëŒ ì†ì„± ë¶„ì„ ì™„ë£Œ: {len(person_detections)}ëª… ë¶„ì„")
            
            return {
                'person_detections': person_detections,
                'person_statistics': person_stats,
                'total_persons': len(person_detections)
            }
            
        except Exception as e:
            print(f"âš ï¸ ì‚¬ëŒ ì†ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'person_detections': [],
                'person_statistics': {},
                'total_persons': 0
            }
    
    def _generate_temporal_statistics(self, video, person_results):
        """ì‹œê°„ë³„ í†µê³„ ìƒì„±"""
        try:
            print(f"ğŸ“Š ë¹„ë””ì˜¤ {video.id} ì‹œê°„ë³„ í†µê³„ ìƒì„± ì‹œì‘")
            
            person_detections = person_results.get('person_detections', [])
            if not person_detections:
                return {'temporal_stats': []}
            
            # ë¹„ë””ì˜¤ë¥¼ 30ì´ˆ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ í†µê³„ ìƒì„±
            duration = video.duration or 300  # ê¸°ë³¸ 5ë¶„
            segment_duration = 30  # 30ì´ˆ ë‹¨ìœ„
            
            temporal_stats = []
            
            for start_time in range(0, int(duration), segment_duration):
                end_time = min(start_time + segment_duration, duration)
                
                # í•´ë‹¹ ì‹œê°„ êµ¬ê°„ì˜ ì‚¬ëŒ ë°ì´í„° í•„í„°ë§
                segment_persons = [
                    p for p in person_detections 
                    if start_time <= p['timestamp'] < end_time
                ]
                
                if not segment_persons:
                    continue
                
                # ì„±ë³„ ë¶„í¬ ê³„ì‚°
                gender_counts = Counter()
                color_counts = Counter()
                
                for person in segment_persons:
                    attrs = person.get('attributes', {})
                    gender = attrs.get('gender_estimation', {}).get('gender', 'unknown')
                    upper_color = attrs.get('upper_body_color', {}).get('color', 'unknown')
                    
                    if gender != 'unknown':
                        gender_counts[gender] += 1
                    if upper_color != 'unknown':
                        color_counts[upper_color] += 1
                
                # TemporalStatistics ëª¨ë¸ì— ì €ì¥
                stats_obj = TemporalStatistics.objects.create(
                    video=video,
                    start_timestamp=start_time,
                    end_timestamp=end_time,
                    duration=end_time - start_time,
                    total_persons_detected=len(segment_persons),
                    male_count=gender_counts.get('male', 0),
                    female_count=gender_counts.get('female', 0),
                    unknown_gender_count=gender_counts.get('unknown', 0),
                    activity_density=len(segment_persons) / segment_duration,
                    top_colors=dict(color_counts.most_common(3)),
                    detailed_statistics={
                        'gender_distribution': dict(gender_counts),
                        'color_distribution': dict(color_counts),
                        'time_range': f"{start_time}-{end_time}s"
                    }
                )
                
                temporal_stats.append({
                    'start_time': start_time,
                    'end_time': end_time,
                    'total_persons': len(segment_persons),
                    'gender_distribution': dict(gender_counts),
                    'top_colors': dict(color_counts.most_common(3)),
                    'activity_density': len(segment_persons) / segment_duration
                })
            
            print(f"âœ… ë¹„ë””ì˜¤ {video.id} ì‹œê°„ë³„ í†µê³„ ìƒì„± ì™„ë£Œ: {len(temporal_stats)}ê°œ êµ¬ê°„")
            
            return {
                'temporal_stats': temporal_stats,
                'total_segments': len(temporal_stats)
            }
            
        except Exception as e:
            print(f"âš ï¸ ì‹œê°„ë³„ í†µê³„ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'temporal_stats': []}
    
    def _analyze_object_tracking(self, video, basic_results):
        """ê°ì²´ ì¶”ì  ë¶„ì„"""
        try:
            print(f"ğŸ¯ ë¹„ë””ì˜¤ {video.id} ê°ì²´ ì¶”ì  ë¶„ì„ ì‹œì‘")
            
            frame_results = basic_results.get('frame_results', [])
            
            # ì¶”ì  IDë³„ ê°ì²´ ê·¸ë£¹í™”
            tracks = defaultdict(list)
            
            for frame_data in frame_results:
                objects = frame_data.get('objects', [])
                timestamp = frame_data.get('timestamp', 0)
                
                for obj in objects:
                    track_id = obj.get('track_id')
                    if track_id is not None:
                        tracks[track_id].append({
                            'timestamp': timestamp,
                            'object_class': obj.get('class'),
                            'bbox': obj.get('bbox', []),
                            'confidence': obj.get('confidence', 0.5)
                        })
            
            # ì¶”ì  ì •ë³´ ë¶„ì„ ë° ì €ì¥
            tracking_results = []
            
            for track_id, detections in tracks.items():
                if len(detections) < 2:  # ìµœì†Œ 2íšŒ ì´ìƒ ê°ì§€ëœ ê²ƒë§Œ
                    continue
                
                # ì‹œê°„ìˆœ ì •ë ¬
                detections.sort(key=lambda x: x['timestamp'])
                
                first_detection = detections[0]
                last_detection = detections[-1]
                
                # ì´ë™ ê²½ë¡œ ê³„ì‚°
                movement_path = []
                total_distance = 0.0
                
                for i, detection in enumerate(detections):
                    bbox = detection['bbox']
                    if len(bbox) == 4:
                        center_x = (bbox[0] + bbox[2]) / 2
                        center_y = (bbox[1] + bbox[3]) / 2
                        movement_path.append([center_x, center_y])
                        
                        # ì´ë™ ê±°ë¦¬ ê³„ì‚°
                        if i > 0:
                            prev_center = movement_path[i-1]
                            distance = ((center_x - prev_center[0])**2 + (center_y - prev_center[1])**2)**0.5
                            total_distance += distance
                
                # í‰ê·  ì†ë„ ê³„ì‚°
                duration = last_detection['timestamp'] - first_detection['timestamp']
                average_speed = total_distance / duration if duration > 0 else 0
                
                # ObjectTracking ëª¨ë¸ì— ì €ì¥
                try:
                    ObjectTracking.objects.create(
                        video=video,
                        track_id=track_id,
                        object_class=first_detection['object_class'],
                        first_appearance=first_detection['timestamp'],
                        last_appearance=last_detection['timestamp'],
                        total_duration=duration,
                        total_detections=len(detections),
                        tracking_confidence=sum(d['confidence'] for d in detections) / len(detections),
                        movement_path=movement_path,
                        movement_distance=total_distance,
                        average_speed=average_speed,
                        tracking_quality='high' if len(detections) > 10 else 'medium' if len(detections) > 5 else 'low'
                    )
                except Exception as save_error:
                    print(f"âš ï¸ ObjectTracking ì €ì¥ ì‹¤íŒ¨: {save_error}")
                
                tracking_results.append({
                    'track_id': track_id,
                    'object_class': first_detection['object_class'],
                    'duration': duration,
                    'detections_count': len(detections),
                    'movement_distance': total_distance,
                    'average_speed': average_speed
                })
            
            print(f"âœ… ë¹„ë””ì˜¤ {video.id} ê°ì²´ ì¶”ì  ë¶„ì„ ì™„ë£Œ: {len(tracking_results)}ê°œ ì¶”ì ")
            
            return {
                'tracks': tracking_results,
                'total_tracks': len(tracking_results)
            }
            
        except Exception as e:
            print(f"âš ï¸ ê°ì²´ ì¶”ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'tracks': []}
    
    def _build_search_index(self, video, all_results):
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            print(f"ğŸ” ë¹„ë””ì˜¤ {video.id} ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘")
            
            basic_results = all_results.get('basic', {})
            metadata_results = all_results.get('metadata', {})
            person_results = all_results.get('person', {})
            
            # ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            searchable_texts = []
            
            # í”„ë ˆì„ ìº¡ì…˜ë“¤
            frame_results = basic_results.get('frame_results', [])
            for frame in frame_results:
                if frame.get('caption'):
                    searchable_texts.append(frame['caption'])
            
            # ê°ì²´ ì •ë³´
            video_summary = basic_results.get('video_summary', {})
            dominant_objects = video_summary.get('dominant_objects', [])
            
            # ì‚¬ëŒ ì†ì„± ì •ë³´
            person_attributes = person_results.get('person_statistics', {})
            
            # ë©”íƒ€ë°ì´í„° íƒœê·¸
            weather_tags = []
            time_tags = []
            location_tags = []
            
            if metadata_results.get('dominant_weather') != 'unknown':
                weather_tags.append(metadata_results['dominant_weather'])
            
            if metadata_results.get('dominant_time_period') != 'unknown':
                time_tags.append(metadata_results['dominant_time_period'])
            
            if metadata_results.get('location_type') != 'unknown':
                location_tags.append(metadata_results['location_type'])
            
            # ì˜ìƒ ìƒ‰ìƒ ì •ë³´
            clothing_colors = []
            person_detections = person_results.get('person_detections', [])
            for person in person_detections:
                attrs = person.get('attributes', {})
                upper_color = attrs.get('upper_body_color', {}).get('color')
                lower_color = attrs.get('lower_body_color', {}).get('color')
                
                if upper_color and upper_color != 'unknown':
                    clothing_colors.append(upper_color)
                if lower_color and lower_color != 'unknown':
                    clothing_colors.append(lower_color)
            
            # ì„±ë³„ ë¶„í¬
            gender_distribution = {}
            if person_attributes:
                gender_counts = person_attributes.get('gender_distribution', {})
                total_persons = sum(gender_counts.values())
                if total_persons > 0:
                    gender_distribution = {
                        'male_percentage': (gender_counts.get('male', 0) / total_persons) * 100,
                        'female_percentage': (gender_counts.get('female', 0) / total_persons) * 100
                    }
            
            # VideoSearchIndex ìƒì„±/ì—…ë°ì´íŠ¸
            VideoSearchIndex.objects.update_or_create(
                video=video,
                defaults={
                    'searchable_text': ' '.join(searchable_texts),
                    'keywords': list(set(searchable_texts[:50])),  # ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ
                    'all_objects': dominant_objects,
                    'dominant_objects': dominant_objects[:10],
                    'object_counts': dict(Counter(dominant_objects)),
                    'person_attributes_summary': person_attributes,
                    'clothing_colors': list(set(clothing_colors)),
                    'gender_distribution': gender_distribution,
                    'weather_tags': weather_tags,
                    'time_tags': time_tags,
                    'location_tags': location_tags,
                    'visual_features': {
                        'brightness': metadata_results.get('overall_brightness', 128),
                        'contrast': metadata_results.get('overall_contrast', 50),
                        'scene_complexity': metadata_results.get('scene_complexity', 0.5)
                    },
                    'scene_complexity_avg': metadata_results.get('scene_complexity', 0.5),
                    'search_weights': {
                        'object_weight': len(dominant_objects) * 0.1,
                        'text_weight': len(searchable_texts) * 0.05,
                        'metadata_weight': (len(weather_tags) + len(time_tags) + len(location_tags)) * 0.2
                    }
                }
            )
            
            print(f"âœ… ë¹„ë””ì˜¤ {video.id} ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
    
    def _integrate_and_save_results(self, video, analysis_type, all_results):
        """ëª¨ë“  ë¶„ì„ ê²°ê³¼ í†µí•© ë° ì €ì¥"""
        try:
            print(f"ğŸ’¾ ë¹„ë””ì˜¤ {video.id} ë¶„ì„ ê²°ê³¼ í†µí•© ë° ì €ì¥ ì‹œì‘")
            
            basic_results = all_results.get('basic', {})
            metadata_results = all_results.get('metadata', {})
            person_results = all_results.get('person', {})
            temporal_results = all_results.get('temporal', {})
            tracking_results = all_results.get('tracking', {})
            
            # VideoAnalysis ëª¨ë¸ ìƒì„±/ì—…ë°ì´íŠ¸
            processing_time = 300  # ì„ì‹œê°’ (ì‹¤ì œë¡œëŠ” ì‹¤í–‰ ì‹œê°„ ê³„ì‚°)
            
            analysis_statistics = {
                'analysis_type': analysis_type,
                'unique_objects': len(basic_results.get('video_summary', {}).get('dominant_objects', [])),
                'total_detections': basic_results.get('total_frames_analyzed', 0),
                'features_used': [
                    'object_detection',
                    'metadata_analysis',
                    'person_attributes',
                    'temporal_statistics',
                    'object_tracking'
                ],
                'scene_types': [metadata_results.get('location_type', 'unknown')],
                'dominant_objects': basic_results.get('video_summary', {}).get('dominant_objects', []),
                
                # ê³ ê¸‰ ë¶„ì„ í”Œë˜ê·¸
                'weather_analysis': True,
                'person_attribute_analysis': True,
                'temporal_analysis': True,
                'object_tracking': True,
                
                # ë©”íƒ€ë°ì´í„° ì •ë³´
                'weather_detected': metadata_results.get('dominant_weather', 'unknown'),
                'time_period_detected': metadata_results.get('dominant_time_period', 'unknown'),
                'location_detected': metadata_results.get('location_type', 'unknown'),
                
                # í†µê³„ ì •ë³´
                'total_persons_detected': person_results.get('total_persons', 0),
                'temporal_segments': temporal_results.get('total_segments', 0),
                'tracking_objects': tracking_results.get('total_tracks', 0)
            }
            
            caption_statistics = {
                'frames_with_caption': basic_results.get('total_frames_analyzed', 0),
                'enhanced_captions': basic_results.get('total_frames_analyzed', 0),
                'text_content_length': len(basic_results.get('video_summary', {}).get('text_content', '')),
                'average_confidence': 0.85,
                
                # ê³ ê¸‰ ìº¡ì…˜ í†µê³„
                'person_attributes_extracted': person_results.get('total_persons', 0),
                'temporal_stats_generated': temporal_results.get('total_segments', 0),
                'weather_confidence': metadata_results.get('weather_confidence', 0.0),
                'time_confidence': metadata_results.get('time_confidence', 0.0)
            }
            
            # VideoAnalysis ê°ì²´ ìƒì„±
            video_analysis = VideoAnalysis.objects.create(
                video=video,
                enhanced_analysis=True,
                success_rate=95.0,
                processing_time_seconds=processing_time,
                analysis_statistics=analysis_statistics,
                caption_statistics=caption_statistics
            )
            
            # ë¹„ë””ì˜¤ ìƒíƒœ ì—…ë°ì´íŠ¸
            video.analysis_status = 'completed'
            video.is_analyzed = True
            video.save()
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            final_results = {
                'success': True,
                'analysis_type': analysis_type,
                'video_analysis_id': video_analysis.id,
                'statistics': analysis_statistics,
                'comprehensive_results': {
                    'metadata': metadata_results,
                    'person_analysis': person_results,
                    'temporal_analysis': temporal_results,
                    'tracking_analysis': tracking_results
                },
                'processing_summary': {
                    'total_frames_analyzed': basic_results.get('total_frames_analyzed', 0),
                    'persons_detected': person_results.get('total_persons', 0),
                    'temporal_segments': temporal_results.get('total_segments', 0),
                    'object_tracks': tracking_results.get('total_tracks', 0),
                    'processing_time_seconds': processing_time,
                    'analysis_features': analysis_statistics['features_used']
                }
            }
            
            print(f"âœ… ë¹„ë””ì˜¤ {video.id} ë¶„ì„ ê²°ê³¼ í†µí•© ë° ì €ì¥ ì™„ë£Œ")
            return final_results
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise e
    
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    
    def _get_video_path(self, video):
        """ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°"""
        possible_paths = [
            os.path.join(settings.MEDIA_ROOT, 'videos', video.filename),
            os.path.join(settings.MEDIA_ROOT, 'uploads', video.filename),
            getattr(video, 'file_path', None)
        ]
        
        for path in possible_paths:
            if path and os.path.exists(path):
                return path
        return None
    
    def _extract_dominant_objects(self, frame_results):
        """í”„ë ˆì„ ê²°ê³¼ì—ì„œ ì£¼ìš” ê°ì²´ ì¶”ì¶œ"""
        all_objects = []
        for frame in frame_results:
            for obj in frame.get('objects', []):
                if obj.get('class'):
                    all_objects.append(obj['class'])
        
        object_counter = Counter(all_objects)
        return [obj for obj, count in object_counter.most_common(20)]
    
    def _generate_person_statistics(self, person_detections):
        """ì‚¬ëŒ ê°ì§€ ê²°ê³¼ì—ì„œ í†µê³„ ìƒì„±"""
        if not person_detections:
            return {}
        
        gender_counts = Counter()
        color_counts = Counter()
        posture_counts = Counter()
        
        for person in person_detections:
            attrs = person.get('attributes', {})
            
            gender = attrs.get('gender_estimation', {}).get('gender', 'unknown')
            upper_color = attrs.get('upper_body_color', {}).get('color', 'unknown')
            posture = attrs.get('posture', {}).get('posture', 'unknown')
            
            if gender != 'unknown':
                gender_counts[gender] += 1
            if upper_color != 'unknown':
                color_counts[upper_color] += 1
            if posture != 'unknown':
                posture_counts[posture] += 1
        
        return {
            'gender_distribution': dict(gender_counts),
            'color_distribution': dict(color_counts),
            'posture_distribution': dict(posture_counts),
            'total_analyzed': len(person_detections)
        }


# ì „ì—­ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤
_pipeline_instance = None

def get_analysis_pipeline():
    """í†µí•© ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _pipeline_instance
    
    if _pipeline_instance is None:
        _pipeline_instance = IntegratedAnalysisPipeline()
    
    return _pipeline_instance