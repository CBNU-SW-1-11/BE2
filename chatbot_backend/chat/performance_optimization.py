# performance_optimization.py - ì„±ëŠ¥ ìµœì í™” ë° ìºì‹± ì‹œìŠ¤í…œ

import os
import json
import time
import hashlib
import pickle
import threading
from datetime import datetime, timedelta
from collections import defaultdict, OrderedDict
from django.core.cache import cache
from django.conf import settings
from django.db import connection
from django.db.models import Prefetch, Q

import numpy as np
from .models import (
    Video, VideoAnalysis, Frame, VideoMetadata, 
    PersonDetection, TemporalStatistics, ObjectTracking, 
    VideoSearchIndex, SearchQuery
)


class AdvancedCacheManager:
    """ê³ ê¸‰ ìºì‹œ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.memory_cache = OrderedDict()
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0
        }
        self.max_memory_items = 1000
        self.cache_lock = threading.RLock()
        
        # ìºì‹œ TTL ì„¤ì • (ì´ˆ)
        self.ttl_config = {
            'video_metadata': 60 * 60,      # 1ì‹œê°„
            'search_results': 30 * 60,      # 30ë¶„
            'analysis_results': 60 * 60,    # 1ì‹œê°„
            'temporal_stats': 45 * 60,      # 45ë¶„
            'person_attributes': 60 * 60,   # 1ì‹œê°„
            'frame_data': 15 * 60,          # 15ë¶„
            'system_status': 5 * 60         # 5ë¶„
        }
        
        print("ğŸ’¾ ê³ ê¸‰ ìºì‹œ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get(self, cache_key, cache_type='default'):
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        with self.cache_lock:
            # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
            if cache_key in self.memory_cache:
                item = self.memory_cache.pop(cache_key)
                
                # TTL í™•ì¸
                if time.time() < item['expires_at']:
                    # ìµœê·¼ ì‚¬ìš© í•­ëª©ìœ¼ë¡œ ì´ë™
                    self.memory_cache[cache_key] = item
                    self.cache_stats['hits'] += 1
                    return item['data']
                else:
                    # ë§Œë£Œëœ í•­ëª© ì œê±°
                    self.cache_stats['evictions'] += 1
            
            # Django ìºì‹œ í™•ì¸
            django_cache_key = f"advanced_cache:{cache_type}:{cache_key}"
            cached_data = cache.get(django_cache_key)
            
            if cached_data:
                # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
                self._set_memory_cache(cache_key, cached_data, cache_type)
                self.cache_stats['hits'] += 1
                return cached_data
            
            self.cache_stats['misses'] += 1
            return None
    
    def set(self, cache_key, data, cache_type='default', ttl=None):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        if ttl is None:
            ttl = self.ttl_config.get(cache_type, 30 * 60)  # ê¸°ë³¸ 30ë¶„
        
        # Django ìºì‹œì— ì €ì¥
        django_cache_key = f"advanced_cache:{cache_type}:{cache_key}"
        cache.set(django_cache_key, data, ttl)
        
        # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
        self._set_memory_cache(cache_key, data, cache_type, ttl)
        
    def _set_memory_cache(self, cache_key, data, cache_type, ttl=None):
        """ë©”ëª¨ë¦¬ ìºì‹œì— ë°ì´í„° ì €ì¥"""
        with self.cache_lock:
            if ttl is None:
                ttl = self.ttl_config.get(cache_type, 30 * 60)
            
            # ìºì‹œ í¬ê¸° ì œí•œ
            if len(self.memory_cache) >= self.max_memory_items:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (LRU)
                oldest_key = next(iter(self.memory_cache))
                del self.memory_cache[oldest_key]
                self.cache_stats['evictions'] += 1
            
            # ìƒˆ í•­ëª© ì¶”ê°€
            self.memory_cache[cache_key] = {
                'data': data,
                'expires_at': time.time() + ttl,
                'cache_type': cache_type
            }
    
    def invalidate(self, pattern=None, cache_type=None):
        """ìºì‹œ ë¬´íš¨í™”"""
        with self.cache_lock:
            # ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
            if pattern:
                keys_to_remove = [k for k in self.memory_cache.keys() if pattern in k]
                for key in keys_to_remove:
                    del self.memory_cache[key]
            elif cache_type:
                keys_to_remove = [
                    k for k, v in self.memory_cache.items() 
                    if v.get('cache_type') == cache_type
                ]
                for key in keys_to_remove:
                    del self.memory_cache[key]
            else:
                self.memory_cache.clear()
        
        # Django ìºì‹œ ì •ë¦¬ëŠ” íŒ¨í„´ ë§¤ì¹­ì´ ì–´ë ¤ìš°ë¯€ë¡œ ì „ì²´ ì •ë¦¬
        if not pattern and not cache_type:
            cache.clear()
    
    def get_stats(self):
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = (self.cache_stats['hits'] / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'memory_cache_size': len(self.memory_cache),
            'cache_stats': self.cache_stats,
            'hit_rate_percentage': round(hit_rate, 2),
            'memory_usage_mb': self._calculate_memory_usage()
        }
    
    def _calculate_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚° (ê·¼ì‚¬ê°’)"""
        try:
            total_size = 0
            for item in self.memory_cache.values():
                # ê°„ë‹¨í•œ í¬ê¸° ì¶”ì •
                total_size += len(pickle.dumps(item['data']))
            return round(total_size / (1024 * 1024), 2)  # MB ë‹¨ìœ„
        except:
            return 0


class DatabaseOptimizer:
    """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ê´€ë¦¬ì"""
    
    def __init__(self):
        self.query_cache = {}
        self.slow_query_threshold = 1.0  # 1ì´ˆ
        self.query_stats = defaultdict(list)
        
    def optimize_video_queries(self):
        """ë¹„ë””ì˜¤ ê´€ë ¨ ì¿¼ë¦¬ ìµœì í™”"""
        
        # ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ë“¤ì— ëŒ€í•œ ìµœì í™”ëœ ë©”ì„œë“œë“¤
        optimized_queries = {
            'analyzed_videos': self._get_analyzed_videos_optimized,
            'video_with_analysis': self._get_video_with_analysis_optimized,
            'person_detections_by_video': self._get_person_detections_optimized,
            'temporal_stats_by_video': self._get_temporal_stats_optimized,
            'search_index_data': self._get_search_index_optimized
        }
        
        return optimized_queries
    
    def _get_analyzed_videos_optimized(self):
        """ë¶„ì„ëœ ë¹„ë””ì˜¤ ëª©ë¡ ìµœì í™” ì¡°íšŒ"""
        cache_key = "analyzed_videos_list"
        cached_result = cache.get(cache_key)
        
        if cached_result:
            return cached_result
        
        # ìµœì í™”ëœ ì¿¼ë¦¬: í•„ìš”í•œ ê´€ë ¨ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°
        videos = Video.objects.filter(is_analyzed=True).select_related(
            'analysis', 'metadata'
        ).prefetch_related(
            Prefetch(
                'analysis',
                queryset=VideoAnalysis.objects.select_related('video')
            )
        ).order_by('-uploaded_at')
        
        # ê²°ê³¼ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        result = []
        for video in videos:
            video_data = {
                'id': video.id,
                'filename': video.filename,
                'original_name': video.original_name,
                'duration': video.duration,
                'is_analyzed': video.is_analyzed,
                'analysis_status': video.analysis_status,
                'uploaded_at': video.uploaded_at.isoformat(),
                'file_size': video.file_size
            }
            
            # ë¶„ì„ ì •ë³´ ì¶”ê°€
            if hasattr(video, 'analysis') and video.analysis:
                analysis = video.analysis
                video_data.update({
                    'enhanced_analysis': analysis.enhanced_analysis,
                    'success_rate': analysis.success_rate,
                    'processing_time': analysis.processing_time_seconds,
                    'analysis_type': analysis.analysis_statistics.get('analysis_type', 'basic'),
                    'unique_objects': analysis.analysis_statistics.get('unique_objects', 0),
                    'advanced_features_used': {
                        'weather_analysis': analysis.analysis_statistics.get('weather_analysis', False),
                        'person_attributes': analysis.analysis_statistics.get('person_attribute_analysis', False),
                        'temporal_analysis': analysis.analysis_statistics.get('temporal_analysis', False),
                        'object_tracking': analysis.analysis_statistics.get('object_tracking', False)
                    }
                })
            
            # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
            if hasattr(video, 'metadata') and video.metadata:
                metadata = video.metadata
                video_data.update({
                    'weather_detected': metadata.dominant_weather,
                    'time_period_detected': metadata.dominant_time_period,
                    'location_type': metadata.location_type
                })
            
            result.append(video_data)
        
        # ìºì‹œì— ì €ì¥ (5ë¶„)
        cache.set(cache_key, result, 300)
        return result
    
    def _get_video_with_analysis_optimized(self, video_id):
        """íŠ¹ì • ë¹„ë””ì˜¤ì˜ ë¶„ì„ ê²°ê³¼ ìµœì í™” ì¡°íšŒ"""
        cache_key = f"video_analysis:{video_id}"
        cached_result = cache.get(cache_key)
        
        if cached_result:
            return cached_result
        
        try:
            # ëª¨ë“  ê´€ë ¨ ë°ì´í„°ë¥¼ í•œ ë²ˆì˜ ì¿¼ë¦¬ë¡œ ê°€ì ¸ì˜¤ê¸°
            video = Video.objects.select_related(
                'analysis', 'metadata'
            ).prefetch_related(
                'temporal_stats',
                'object_tracks',
                'search_index',
                Prefetch(
                    'frames',
                    queryset=Frame.objects.order_by('timestamp')[:50]  # ìµœëŒ€ 50ê°œ í”„ë ˆì„
                ),
                Prefetch(
                    'scenes',
                    queryset=Scene.objects.order_by('scene_id')
                )
            ).get(id=video_id)
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                'video': {
                    'id': video.id,
                    'original_name': video.original_name,
                    'duration': video.duration,
                    'is_analyzed': video.is_analyzed,
                    'analysis_status': video.analysis_status
                },
                'analysis': None,
                'metadata': None,
                'frames_sample': [],
                'scenes': [],
                'temporal_stats': [],
                'object_tracks': [],
                'search_index': None
            }
            
            # ë¶„ì„ ê²°ê³¼
            if hasattr(video, 'analysis') and video.analysis:
                result['analysis'] = {
                    'enhanced_analysis': video.analysis.enhanced_analysis,
                    'success_rate': video.analysis.success_rate,
                    'processing_time_seconds': video.analysis.processing_time_seconds,
                    'analysis_statistics': video.analysis.analysis_statistics,
                    'caption_statistics': video.analysis.caption_statistics
                }
            
            # ë©”íƒ€ë°ì´í„°
            if hasattr(video, 'metadata') and video.metadata:
                result['metadata'] = {
                    'dominant_weather': video.metadata.dominant_weather,
                    'weather_confidence': video.metadata.weather_confidence,
                    'dominant_time_period': video.metadata.dominant_time_period,
                    'time_confidence': video.metadata.time_confidence,
                    'location_type': video.metadata.location_type,
                    'overall_brightness': video.metadata.overall_brightness
                }
            
            # í”„ë ˆì„ ìƒ˜í”Œ
            result['frames_sample'] = [
                {
                    'image_id': frame.image_id,
                    'timestamp': frame.timestamp,
                    'caption': frame.final_caption or frame.enhanced_caption or frame.caption,
                    'detected_objects': frame.detected_objects
                }
                for frame in video.frames.all()
            ]
            
            # ì”¬ ì •ë³´
            result['scenes'] = [
                {
                    'scene_id': scene.scene_id,
                    'start_time': scene.start_time,
                    'end_time': scene.end_time,
                    'dominant_objects': scene.dominant_objects
                }
                for scene in video.scenes.all()
            ]
            
            # ì‹œê°„ë³„ í†µê³„
            result['temporal_stats'] = [
                {
                    'start_timestamp': stat.start_timestamp,
                    'end_timestamp': stat.end_timestamp,
                    'male_count': stat.male_count,
                    'female_count': stat.female_count,
                    'activity_density': stat.activity_density,
                    'top_colors': stat.top_colors
                }
                for stat in video.temporal_stats.all()
            ]
            
            # ê°ì²´ ì¶”ì 
            result['object_tracks'] = [
                {
                    'track_id': track.track_id,
                    'object_class': track.object_class,
                    'first_appearance': track.first_appearance,
                    'last_appearance': track.last_appearance,
                    'total_duration': track.total_duration,
                    'movement_distance': track.movement_distance
                }
                for track in video.object_tracks.all()
            ]
            
            # ê²€ìƒ‰ ì¸ë±ìŠ¤
            if hasattr(video, 'search_index') and video.search_index:
                result['search_index'] = {
                    'dominant_objects': video.search_index.dominant_objects,
                    'weather_tags': video.search_index.weather_tags,
                    'time_tags': video.search_index.time_tags,
                    'location_tags': video.search_index.location_tags,
                    'clothing_colors': video.search_index.clothing_colors
                }
            
            # ìºì‹œì— ì €ì¥ (30ë¶„)
            cache.set(cache_key, result, 1800)
            return result
            
        except Video.DoesNotExist:
            return None
    
    def _get_person_detections_optimized(self, video_id, filters=None):
        """ì‚¬ëŒ ê°ì§€ ë°ì´í„° ìµœì í™” ì¡°íšŒ"""
        cache_key = f"person_detections:{video_id}:{hash(str(filters))}"
        cached_result = cache.get(cache_key)
        
        if cached_result:
            return cached_result
        
        # ê¸°ë³¸ ì¿¼ë¦¬
        queryset = PersonDetection.objects.filter(
            frame__video_id=video_id
        ).select_related('frame').order_by('frame__timestamp')
        
        # í•„í„° ì ìš©
        if filters:
            if 'gender' in filters:
                queryset = queryset.filter(gender_estimation=filters['gender'])
            if 'min_confidence' in filters:
                queryset = queryset.filter(confidence__gte=filters['min_confidence'])
            if 'color' in filters:
                queryset = queryset.filter(
                    Q(upper_body_color=filters['color']) | 
                    Q(lower_body_color=filters['color'])
                )
        
        # ê²°ê³¼ êµ¬ì„±
        result = []
        for detection in queryset:
            result.append({
                'frame_id': detection.frame.image_id,
                'timestamp': detection.frame.timestamp,
                'person_id': detection.person_id,
                'track_id': detection.track_id,
                'bbox': [
                    detection.bbox_x1, detection.bbox_y1,
                    detection.bbox_x2, detection.bbox_y2
                ],
                'confidence': detection.confidence,
                'gender': detection.gender_estimation,
                'gender_confidence': detection.gender_confidence,
                'upper_body_color': detection.upper_body_color,
                'lower_body_color': detection.lower_body_color,
                'posture': detection.posture
            })
        
        # ìºì‹œì— ì €ì¥ (15ë¶„)
        cache.set(cache_key, result, 900)
        return result
    
    def _get_temporal_stats_optimized(self, video_id, time_range=None):
        """ì‹œê°„ë³„ í†µê³„ ìµœì í™” ì¡°íšŒ"""
        cache_key = f"temporal_stats:{video_id}:{hash(str(time_range))}"
        cached_result = cache.get(cache_key)
        
        if cached_result:
            return cached_result
        
        # ê¸°ë³¸ ì¿¼ë¦¬
        queryset = TemporalStatistics.objects.filter(
            video_id=video_id
        ).order_by('start_timestamp')
        
        # ì‹œê°„ ë²”ìœ„ í•„í„° ì ìš©
        if time_range:
            start_time = time_range.get('start', 0)
            end_time = time_range.get('end', float('inf'))
            queryset = queryset.filter(
                start_timestamp__gte=start_time,
                end_timestamp__lte=end_time
            )
        
        # ê²°ê³¼ êµ¬ì„±
        result = []
        for stat in queryset:
            result.append({
                'start_timestamp': stat.start_timestamp,
                'end_timestamp': stat.end_timestamp,
                'duration': stat.duration,
                'total_persons_detected': stat.total_persons_detected,
                'male_count': stat.male_count,
                'female_count': stat.female_count,
                'activity_density': stat.activity_density,
                'top_colors': stat.top_colors,
                'detailed_statistics': stat.detailed_statistics
            })
        
        # ìºì‹œì— ì €ì¥ (20ë¶„)
        cache.set(cache_key, result, 1200)
        return result
    
    def _get_search_index_optimized(self, video_ids=None):
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ ìµœì í™” ì¡°íšŒ"""
        cache_key = f"search_index:{hash(str(video_ids))}"
        cached_result = cache.get(cache_key)
        
        if cached_result:
            return cached_result
        
        # ê¸°ë³¸ ì¿¼ë¦¬
        queryset = VideoSearchIndex.objects.select_related('video')
        
        if video_ids:
            queryset = queryset.filter(video_id__in=video_ids)
        
        # ê²°ê³¼ êµ¬ì„±
        result = []
        for index in queryset:
            result.append({
                'video_id': index.video.id,
                'video_name': index.video.original_name,
                'dominant_objects': index.dominant_objects,
                'weather_tags': index.weather_tags,
                'time_tags': index.time_tags,
                'location_tags': index.location_tags,
                'clothing_colors': index.clothing_colors,
                'gender_distribution': index.gender_distribution,
                'scene_complexity_avg': index.scene_complexity_avg,
                'search_weights': index.search_weights
            })
        
        # ìºì‹œì— ì €ì¥ (10ë¶„)
        cache.set(cache_key, result, 600)
        return result
    
    def log_slow_query(self, query, execution_time):
        """ëŠë¦° ì¿¼ë¦¬ ë¡œê¹…"""
        if execution_time > self.slow_query_threshold:
            self.query_stats['slow_queries'].append({
                'query': str(query)[:200],  # ì²˜ìŒ 200ìë§Œ
                'execution_time': execution_time,
                'timestamp': datetime.now().isoformat()
            })
            
            # ìµœëŒ€ 100ê°œë§Œ ìœ ì§€
            if len(self.query_stats['slow_queries']) > 100:
                self.query_stats['slow_queries'] = self.query_stats['slow_queries'][-100:]
    
    def get_performance_stats(self):
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        return {
            'slow_queries_count': len(self.query_stats.get('slow_queries', [])),
            'slow_query_threshold': self.slow_query_threshold,
            'recent_slow_queries': self.query_stats.get('slow_queries', [])[-10:],
            'database_connections': len(connection.queries) if hasattr(connection, 'queries') else 0
        }


class SearchOptimizer:
    """ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™”"""
    
    def __init__(self, cache_manager):
        self.cache_manager = cache_manager
        self.search_stats = defaultdict(int)
        
    def optimize_search_query(self, search_conditions, video_data):
        """ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”"""
        
        # ê²€ìƒ‰ ì¡°ê±´ í•´ì‹œ ìƒì„±
        conditions_hash = hashlib.md5(
            json.dumps(search_conditions, sort_keys=True).encode()
        ).hexdigest()[:16]
        
        cache_key = f"search_optimized:{conditions_hash}"
        
        # ìºì‹œëœ ê²°ê³¼ í™•ì¸
        cached_result = self.cache_manager.get(cache_key, 'search_results')
        if cached_result:
            self.search_stats['cache_hits'] += 1
            return cached_result
        
        # ê²€ìƒ‰ ì¡°ê±´ì— ë”°ë¥¸ ìµœì í™”ëœ í•„í„° ì ìš©
        optimized_results = self._apply_optimized_filters(search_conditions, video_data)
        
        # ê²°ê³¼ ìºì‹±
        self.cache_manager.set(cache_key, optimized_results, 'search_results')
        self.search_stats['cache_misses'] += 1
        
        return optimized_results
    
    def _apply_optimized_filters(self, conditions, video_data):
        """ìµœì í™”ëœ í•„í„° ì ìš©"""
        
        # ì¡°ê±´ë³„ ê°€ì¤‘ì¹˜ ì„¤ì •
        condition_weights = {
            'weather': 0.4,
            'time_period': 0.3,
            'objects': 0.3,
            'location_type': 0.2
        }
        
        matching_videos = []
        
        for video in video_data:
            match_score = 0.0
            match_reasons = []
            
            # ë‚ ì”¨ ì¡°ê±´ ë§¤ì¹­ (ìµœì í™”ëœ ë¡œì§)
            if 'weather' in conditions:
                weather_score = self._calculate_weather_match(
                    conditions['weather'], 
                    video.get('metadata', {})
                )
                match_score += weather_score * condition_weights['weather']
                if weather_score > 0.5:
                    match_reasons.append(f"ë‚ ì”¨: {conditions['weather']}")
            
            # ì‹œê°„ëŒ€ ì¡°ê±´ ë§¤ì¹­
            if 'time_period' in conditions:
                time_score = self._calculate_time_match(
                    conditions['time_period'],
                    video.get('metadata', {})
                )
                match_score += time_score * condition_weights['time_period']
                if time_score > 0.5:
                    match_reasons.append(f"ì‹œê°„ëŒ€: {conditions['time_period']}")
            
            # ê°ì²´ ì¡°ê±´ ë§¤ì¹­ (ìµœì í™”)
            if 'objects' in conditions:
                object_score = self._calculate_object_match(
                    conditions['objects'],
                    video.get('dominant_objects', [])
                )
                match_score += object_score * condition_weights['objects']
                if object_score > 0.3:
                    match_reasons.append("ê°ì²´ ë§¤ì¹­")
            
            # ì¥ì†Œ ì¡°ê±´ ë§¤ì¹­
            if 'location_type' in conditions:
                location_score = self._calculate_location_match(
                    conditions['location_type'],
                    video.get('metadata', {})
                )
                match_score += location_score * condition_weights['location_type']
                if location_score > 0.5:
                    match_reasons.append(f"ì¥ì†Œ: {conditions['location_type']}")
            
            # ì„ê³„ê°’ í™•ì¸
            match_threshold = conditions.get('match_threshold', 0.3)
            if match_score >= match_threshold:
                matching_videos.append({
                    'video_id': video['video_id'],
                    'video_name': video['video_name'],
                    'match_score': match_score,
                    'match_reasons': match_reasons
                })
        
        # ë§¤ì¹­ ì ìˆ˜ë¡œ ì •ë ¬
        matching_videos.sort(key=lambda x: x['match_score'], reverse=True)
        
        return matching_videos
    
    def _calculate_weather_match(self, target_weather, metadata):
        """ë‚ ì”¨ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (ìµœì í™”)"""
        if not metadata:
            return 0.0
            
        detected_weather = metadata.get('dominant_weather', 'unknown')
        confidence = metadata.get('weather_confidence', 0.0)
        
        if detected_weather == target_weather:
            return confidence
        elif detected_weather == 'unknown':
            return 0.1  # ë¶ˆí™•ì‹¤í•œ ê²½ìš° ë‚®ì€ ì ìˆ˜
        else:
            return 0.0
    
    def _calculate_time_match(self, target_time, metadata):
        """ì‹œê°„ëŒ€ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        if not metadata:
            return 0.0
            
        detected_time = metadata.get('dominant_time_period', 'unknown')
        confidence = metadata.get('time_confidence', 0.0)
        
        if detected_time == target_time:
            return confidence
        elif detected_time == 'unknown':
            return 0.1
        else:
            return 0.0
    
    def _calculate_object_match(self, target_objects, video_objects):
        """ê°ì²´ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        if not target_objects or not video_objects:
            return 0.0
        
        target_set = set(target_objects)
        video_set = set(video_objects)
        
        intersection = target_set & video_set
        union = target_set | video_set
        
        # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
        jaccard_score = len(intersection) / len(union) if union else 0.0
        
        return jaccard_score
    
    def _calculate_location_match(self, target_location, metadata):
        """ì¥ì†Œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        if not metadata:
            return 0.0
            
        detected_location = metadata.get('location_type', 'unknown')
        confidence = metadata.get('location_confidence', 0.0)
        
        if detected_location == target_location:
            return confidence
        elif detected_location == 'unknown':
            return 0.1
        else:
            return 0.0
    
    def get_search_stats(self):
        """ê²€ìƒ‰ í†µê³„ ë°˜í™˜"""
        total_searches = self.search_stats['cache_hits'] + self.search_stats['cache_misses']
        cache_hit_rate = (self.search_stats['cache_hits'] / total_searches * 100) if total_searches > 0 else 0
        
        return {
            'total_searches': total_searches,
            'cache_hits': self.search_stats['cache_hits'],
            'cache_misses': self.search_stats['cache_misses'],
            'cache_hit_rate_percentage': round(cache_hit_rate, 2)
        }


class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.metrics = defaultdict(list)
        self.alerts = []
        self.thresholds = {
            'response_time_ms': 5000,    # 5ì´ˆ
            'memory_usage_mb': 1024,     # 1GB
            'cache_hit_rate': 70,        # 70%
            'error_rate': 5              # 5%
        }
    
    def record_metric(self, metric_name, value, timestamp=None):
        """ë©”íŠ¸ë¦­ ê¸°ë¡"""
        if timestamp is None:
            timestamp = time.time()
        
        self.metrics[metric_name].append({
            'value': value,
            'timestamp': timestamp
        })
        
        # ìµœëŒ€ 1000ê°œ ë°ì´í„° í¬ì¸íŠ¸ë§Œ ìœ ì§€
        if len(self.metrics[metric_name]) > 1000:
            self.metrics[metric_name] = self.metrics[metric_name][-1000:]
        
        # ì„ê³„ê°’ í™•ì¸
        self._check_threshold(metric_name, value)
    
    def _check_threshold(self, metric_name, value):
        """ì„ê³„ê°’ í™•ì¸ ë° ì•Œë¦¼ ìƒì„±"""
        if metric_name in self.thresholds:
            threshold = self.thresholds[metric_name]
            
            if (metric_name == 'cache_hit_rate' and value < threshold) or \
               (metric_name != 'cache_hit_rate' and value > threshold):
                
                alert = {
                    'metric': metric_name,
                    'value': value,
                    'threshold': threshold,
                    'timestamp': datetime.now().isoformat(),
                    'severity': 'warning' if value < threshold * 1.2 else 'critical'
                }
                
                self.alerts.append(alert)
                
                # ìµœëŒ€ 50ê°œ ì•Œë¦¼ë§Œ ìœ ì§€
                if len(self.alerts) > 50:
                    self.alerts = self.alerts[-50:]
    
    def get_performance_summary(self):
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        summary = {}
        
        for metric_name, data_points in self.metrics.items():
            if data_points:
                values = [dp['value'] for dp in data_points[-10:]]  # ìµœê·¼ 10ê°œ
                summary[metric_name] = {
                    'current': values[-1] if values else 0,
                    'average': sum(values) / len(values) if values else 0,
                    'min': min(values) if values else 0,
                    'max': max(values) if values else 0,
                    'trend': 'improving' if len(values) >= 2 and values[-1] < values[0] else 'stable'
                }
        
        return {
            'metrics': summary,
            'recent_alerts': self.alerts[-5:],  # ìµœê·¼ 5ê°œ ì•Œë¦¼
            'total_alerts': len(self.alerts),
            'system_health': self._calculate_system_health()
        }
    
    def _calculate_system_health(self):
        """ì‹œìŠ¤í…œ ê±´ê°•ë„ ê³„ì‚°"""
        health_score = 100
        
        # ìµœê·¼ ì•Œë¦¼ ê°œìˆ˜ì— ë”°ë¥¸ ì ìˆ˜ ì°¨ê°
        recent_alerts = len([a for a in self.alerts if 
                           datetime.fromisoformat(a['timestamp']) > datetime.now() - timedelta(hours=1)])
        
        health_score -= recent_alerts * 10
        
        # ìºì‹œ íˆíŠ¸ìœ¨ì— ë”°ë¥¸ ì ìˆ˜ ì¡°ì •
        if 'cache_hit_rate' in self.metrics and self.metrics['cache_hit_rate']:
            cache_hit_rate = self.metrics['cache_hit_rate'][-1]['value']
            if cache_hit_rate < 50:
                health_score -= 20
            elif cache_hit_rate < 70:
                health_score -= 10
        
        return max(0, min(100, health_score))


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ë“¤
cache_manager = AdvancedCacheManager()
db_optimizer = DatabaseOptimizer()
search_optimizer = SearchOptimizer(cache_manager)
performance_monitor = PerformanceMonitor()

# ì„±ëŠ¥ ìµœì í™” ë°ì½”ë ˆì´í„°
def cache_result(cache_type='default', ttl=None):
    """ê²°ê³¼ ìºì‹± ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"
            
            # ìºì‹œëœ ê²°ê³¼ í™•ì¸
            cached_result = cache_manager.get(cache_key, cache_type)
            if cached_result is not None:
                return cached_result
            
            # í•¨ìˆ˜ ì‹¤í–‰
            start_time = time.time()
            result = func(*args, **kwargs)
            execution_time = time.time() - start_time
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
            performance_monitor.record_metric(f"{func.__name__}_execution_time", execution_time * 1000)
            
            # ê²°ê³¼ ìºì‹±
            cache_manager.set(cache_key, result, cache_type, ttl)
            
            return result
        return wrapper
    return decorator

def monitor_performance(metric_prefix=''):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            start_time = time.time()
            
            try:
                result = func(*args, **kwargs)
                execution_time = (time.time() - start_time) * 1000
                
                # ì„±ê³µ ë©”íŠ¸ë¦­ ê¸°ë¡
                performance_monitor.record_metric(f"{metric_prefix}{func.__name__}_response_time", execution_time)
                performance_monitor.record_metric(f"{metric_prefix}{func.__name__}_success", 1)
                
                return result
                
            except Exception as e:
                execution_time = (time.time() - start_time) * 1000
                
                # ì‹¤íŒ¨ ë©”íŠ¸ë¦­ ê¸°ë¡
                performance_monitor.record_metric(f"{metric_prefix}{func.__name__}_response_time", execution_time)
                performance_monitor.record_metric(f"{metric_prefix}{func.__name__}_error", 1)
                
                raise e
                
        return wrapper
    return decorator