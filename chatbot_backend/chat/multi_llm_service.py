# chat/multi_llm_service.py - ì™„ì „ êµ¬í˜„ ë²„ì „
import asyncio
import json
import time
import os
import base64
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import cv2
import numpy as np
from PIL import Image
import io

# API í´ë¼ì´ì–¸íŠ¸ë“¤ - ì•ˆì „í•œ import
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜")

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    print("âš ï¸ Anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜")

try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    print("âš ï¸ Groq ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜")

try:
    import google.generativeai as genai
    GOOGLE_AVAILABLE = True
except ImportError:
    GOOGLE_AVAILABLE = False
    print("âš ï¸ Google AI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜")

logger = logging.getLogger(__name__)

@dataclass
class LLMResponse:
    model_name: str
    response_text: str
    confidence_score: float
    processing_time: float
    token_usage: Dict[str, int]
    error: str = None
    success: bool = True

class MultiLLMVideoAnalyzer:
    """ë©€í‹° LLM ì˜ìƒ ë¶„ì„ ì„œë¹„ìŠ¤ - ì™„ì „ êµ¬í˜„ ë²„ì „"""
    
    def __init__(self):
        print("ğŸš€ MultiLLMVideoAnalyzer ì´ˆê¸°í™” ì¤‘...")
        
        # API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì•ˆì „í•˜ê²Œ)
        self.openai_client = None
        self.anthropic_client = None
        self.groq_client = None
        self.gemini_model = None
        
        self.available_models = []
        
        # OpenAI ì´ˆê¸°í™”
        if OPENAI_AVAILABLE and os.getenv("OPENAI_API_KEY"):
            try:
                self.openai_client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
                self.available_models.append("gpt-4v")
                print("âœ… OpenAI GPT-4V í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Anthropic ì´ˆê¸°í™”
        if ANTHROPIC_AVAILABLE and os.getenv("ANTHROPIC_API_KEY"):
            try:
                self.anthropic_client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
                self.available_models.append("claude-3.5")
                print("âœ… Anthropic Claude-3.5 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ Anthropic ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Groq ì´ˆê¸°í™” (í…ìŠ¤íŠ¸ ì „ìš©)
        if GROQ_AVAILABLE and os.getenv("GROQ_API_KEY"):
            try:
                self.groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))
                self.available_models.append("groq-llama")
                print("âœ… Groq Llama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ Groq ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Google Gemini ì´ˆê¸°í™”
        if GOOGLE_AVAILABLE and os.getenv("GOOGLE_API_KEY"):
            try:
                genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
                self.gemini_model = genai.GenerativeModel('gemini-1.5-pro-vision-latest')
                self.available_models.append("gemini-pro")
                print("âœ… Google Gemini Pro Vision í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ Google Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        print(f"ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {self.available_models}")
        
        if not self.available_models:
            print("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    def analyze_video_multi_llm(
        self, 
        frame_images: List[str], 
        question: str, 
        video_context: Dict = None
    ) -> Dict[str, LLMResponse]:
        """
        ì—¬ëŸ¬ LLMìœ¼ë¡œ ë™ì‹œì— ì˜ìƒ ë¶„ì„ (ë™ê¸° ë²„ì „)
        """
        print(f"ğŸš€ ë©€í‹° LLM ë¶„ì„ ì‹œì‘: '{question}'")
        print(f"ğŸ“Š ë¶„ì„í•  í”„ë ˆì„ ìˆ˜: {len(frame_images)}")
        print(f"ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {self.available_models}")
        
        if not self.available_models:
            return {
                "error": LLMResponse(
                    model_name="system",
                    response_text="ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.",
                    confidence_score=0.0,
                    processing_time=0.0,
                    token_usage={},
                    error="No models available",
                    success=False
                )
            }
        
        results = {}
        start_time = time.time()
        
        # ê° ëª¨ë¸ë³„ ë¶„ì„ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=len(self.available_models)) as executor:
            future_to_model = {}
            
            # GPT-4V
            if "gpt-4v" in self.available_models:
                future_to_model[executor.submit(self._analyze_with_gpt4v, frame_images, question, video_context)] = "gpt-4v"
            
            # Claude-3.5
            if "claude-3.5" in self.available_models:
                future_to_model[executor.submit(self._analyze_with_claude, frame_images, question, video_context)] = "claude-3.5"
            
            # Groq (í…ìŠ¤íŠ¸ ê¸°ë°˜)
            if "groq-llama" in self.available_models:
                future_to_model[executor.submit(self._analyze_with_groq, frame_images, question, video_context)] = "groq-llama"
            
            # Gemini Pro
            if "gemini-pro" in self.available_models:
                future_to_model[executor.submit(self._analyze_with_gemini, frame_images, question, video_context)] = "gemini-pro"
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_model, timeout=60):
                model_name = future_to_model[future]
                try:
                    result = future.result(timeout=30)
                    results[model_name] = result
                    print(f"âœ… {model_name} ë¶„ì„ ì™„ë£Œ ({result.processing_time:.1f}ì´ˆ)")
                except Exception as e:
                    print(f"âŒ {model_name} ë¶„ì„ ì‹¤íŒ¨: {e}")
                    results[model_name] = LLMResponse(
                        model_name=model_name,
                        response_text="",
                        confidence_score=0.0,
                        processing_time=0.0,
                        token_usage={},
                        error=str(e),
                        success=False
                    )
        
        total_time = time.time() - start_time
        print(f"ğŸ¯ ë©€í‹° LLM ë¶„ì„ ì™„ë£Œ: {total_time:.1f}ì´ˆ")
        
        return results
    
    def _analyze_with_gpt4v(self, frame_images, question, video_context):
        """GPT-4V ë¶„ì„"""
        start_time = time.time()
        
        if not self.openai_client:
            return LLMResponse(
                model_name="gpt-4v",
                response_text="",
                confidence_score=0.0,
                processing_time=0.0,
                token_usage={},
                error="OpenAI client not available",
                success=False
            )
        
        try:
            messages = self._build_gpt_messages(frame_images, question, video_context)
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",  # ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ mini ì‚¬ìš©
                messages=messages,
                max_tokens=1000,
                temperature=0.7
            )
            
            response_text = response.choices[0].message.content
            processing_time = time.time() - start_time
            
            return LLMResponse(
                model_name="gpt-4v",
                response_text=response_text,
                confidence_score=self._calculate_confidence(response_text),
                processing_time=processing_time,
                token_usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens
                },
                success=True
            )
            
        except Exception as e:
            return LLMResponse(
                model_name="gpt-4v",
                response_text="",
                confidence_score=0.0,
                processing_time=time.time() - start_time,
                token_usage={},
                error=str(e),
                success=False
            )
    
    def _analyze_with_claude(self, frame_images, question, video_context):
        """Claude-3.5 ë¶„ì„"""
        start_time = time.time()
        
        if not self.anthropic_client:
            return LLMResponse(
                model_name="claude-3.5",
                response_text="",
                confidence_score=0.0,
                processing_time=0.0,
                token_usage={},
                error="Anthropic client not available",
                success=False
            )
        
        try:
            content = self._build_claude_content(frame_images, question, video_context)
            
            message = self.anthropic_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1000,
                temperature=0.7,
                messages=[{
                    "role": "user",
                    "content": content
                }]
            )
            
            response_text = message.content[0].text
            processing_time = time.time() - start_time
            
            return LLMResponse(
                model_name="claude-3.5",
                response_text=response_text,
                confidence_score=self._calculate_confidence(response_text),
                processing_time=processing_time,
                token_usage={
                    "input_tokens": message.usage.input_tokens,
                    "output_tokens": message.usage.output_tokens
                },
                success=True
            )
            
        except Exception as e:
            return LLMResponse(
                model_name="claude-3.5",
                response_text="",
                confidence_score=0.0,
                processing_time=time.time() - start_time,
                token_usage={},
                error=str(e),
                success=False
            )
    
    def _analyze_with_groq(self, frame_images, question, video_context):
        """Groq ë¶„ì„ (í…ìŠ¤íŠ¸ ê¸°ë°˜)"""
        start_time = time.time()
        
        if not self.groq_client:
            return LLMResponse(
                model_name="groq-llama",
                response_text="",
                confidence_score=0.0,
                processing_time=0.0,
                token_usage={},
                error="Groq client not available",
                success=False
            )
        
        try:
            # GroqëŠ” ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í…ìŠ¤íŠ¸ ì„¤ëª… ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
            prompt = self._build_groq_prompt(question, video_context)
            
            response = self.groq_client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            response_text = response.choices[0].message.content
            processing_time = time.time() - start_time
            
            return LLMResponse(
                model_name="groq-llama",
                response_text=response_text,
                confidence_score=self._calculate_confidence(response_text),
                processing_time=processing_time,
                token_usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens
                },
                success=True
            )
            
        except Exception as e:
            return LLMResponse(
                model_name="groq-llama",
                response_text="",
                confidence_score=0.0,
                processing_time=time.time() - start_time,
                token_usage={},
                error=str(e),
                success=False
            )
    
    def _analyze_with_gemini(self, frame_images, question, video_context):
        """Gemini Pro Vision ë¶„ì„"""
        start_time = time.time()
        
        if not self.gemini_model:
            return LLMResponse(
                model_name="gemini-pro",
                response_text="",
                confidence_score=0.0,
                processing_time=0.0,
                token_usage={},
                error="Gemini model not available",
                success=False
            )
        
        try:
            prompt = self._build_gemini_prompt(question, video_context)
            
            # ì²« ë²ˆì§¸ í”„ë ˆì„ë§Œ ì‚¬ìš© (Gemini ì œí•œ)
            if frame_images:
                image_data = base64.b64decode(frame_images[0])
                
                response = self.gemini_model.generate_content([
                    prompt,
                    {"mime_type": "image/jpeg", "data": image_data}
                ])
                
                response_text = response.text
            else:
                # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ì²˜ë¦¬
                response = self.gemini_model.generate_content(prompt)
                response_text = response.text
            
            processing_time = time.time() - start_time
            
            return LLMResponse(
                model_name="gemini-pro",
                response_text=response_text,
                confidence_score=self._calculate_confidence(response_text),
                processing_time=processing_time,
                token_usage={
                    "prompt_tokens": 0,  # GeminiëŠ” ìƒì„¸ í† í° ì •ë³´ ì œê³µ ì•ˆí•¨
                    "completion_tokens": len(response_text.split())
                },
                success=True
            )
            
        except Exception as e:
            return LLMResponse(
                model_name="gemini-pro",
                response_text="",
                confidence_score=0.0,
                processing_time=time.time() - start_time,
                token_usage={},
                error=str(e),
                success=False
            )
    
    def _build_gpt_messages(self, frame_images, question, video_context):
        """GPT-4Vìš© ë©”ì‹œì§€ êµ¬ì„±"""
        messages = [{
            "role": "system",
            "content": "ë‹¹ì‹ ì€ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì˜ìƒ í”„ë ˆì„ë“¤ì„ ë¶„ì„í•˜ê³  ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        }]
        
        content = [{"type": "text", "text": f"ì˜ìƒ ë¶„ì„ ì§ˆë¬¸: {question}"}]
        
        # ë¹„ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        if video_context:
            context_text = "\n\nì¶”ê°€ ì •ë³´:\n"
            if video_context.get('duration'):
                context_text += f"- ì˜ìƒ ê¸¸ì´: {video_context['duration']}ì´ˆ\n"
            if video_context.get('detected_objects'):
                context_text += f"- ê°ì§€ëœ ê°ì²´: {', '.join(video_context['detected_objects'][:5])}\n"
            if video_context.get('ocr_text'):
                context_text += f"- ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {video_context['ocr_text']}\n"
            
            content[0]["text"] += context_text
        
        # í”„ë ˆì„ ì´ë¯¸ì§€ë“¤ ì¶”ê°€ (ìµœëŒ€ 3ê°œ)
        for i, frame_image in enumerate(frame_images[:3]):
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{frame_image}",
                    "detail": "high"
                }
            })
        
        messages.append({"role": "user", "content": content})
        return messages
    
    def _build_claude_content(self, frame_images, question, video_context):
        """Claudeìš© ì»¨í…ì¸  êµ¬ì„±"""
        content = []
        
        prompt_text = f"ì˜ìƒ ë¶„ì„ ì§ˆë¬¸: {question}\n\n"
        
        if video_context:
            prompt_text += "ì¶”ê°€ ì •ë³´:\n"
            if video_context.get('duration'):
                prompt_text += f"- ì˜ìƒ ê¸¸ì´: {video_context['duration']}ì´ˆ\n"
            if video_context.get('detected_objects'):
                prompt_text += f"- ê°ì§€ëœ ê°ì²´: {', '.join(video_context['detected_objects'][:5])}\n"
            if video_context.get('ocr_text'):
                prompt_text += f"- ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {video_context['ocr_text']}\n"
        
        prompt_text += "\nì£¼ì–´ì§„ ì˜ìƒ í”„ë ˆì„ë“¤ì„ ë¶„ì„í•˜ê³  ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”:"
        
        content.append({"type": "text", "text": prompt_text})
        
        # ì´ë¯¸ì§€ë“¤ ì¶”ê°€ (ìµœëŒ€ 3ê°œ)
        for frame_image in frame_images[:3]:
            content.append({
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/jpeg",
                    "data": frame_image
                }
            })
        
        return content
    
    def _build_groq_prompt(self, question, video_context):
        """Groqìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„± (í…ìŠ¤íŠ¸ ê¸°ë°˜)"""
        prompt = f"ì˜ìƒ ë¶„ì„ ì§ˆë¬¸: {question}\n\n"
        
        if video_context:
            prompt += "ì˜ìƒ ì •ë³´:\n"
            if video_context.get('duration'):
                prompt += f"- ì˜ìƒ ê¸¸ì´: {video_context['duration']}ì´ˆ\n"
            if video_context.get('detected_objects'):
                prompt += f"- ê°ì§€ëœ ê°ì²´ë“¤: {', '.join(video_context['detected_objects'][:10])}\n"
            if video_context.get('ocr_text'):
                prompt += f"- ì˜ìƒ ë‚´ í…ìŠ¤íŠ¸: {video_context['ocr_text']}\n"
            if video_context.get('filename'):
                prompt += f"- íŒŒì¼ëª…: {video_context['filename']}\n"
        
        prompt += "\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ìƒì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ì˜ìƒì„ ì§ì ‘ ë³¼ ìˆ˜ëŠ” ì—†ì§€ë§Œ, ì œê³µëœ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ìµœëŒ€í•œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."
        
        return prompt
    
    def _build_gemini_prompt(self, question, video_context):
        """Geminiìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        prompt = f"ì˜ìƒ ë¶„ì„ ì§ˆë¬¸: {question}\n\n"
        
        if video_context:
            prompt += "ì¶”ê°€ ì •ë³´:\n"
            if video_context.get('duration'):
                prompt += f"- ì˜ìƒ ê¸¸ì´: {video_context['duration']}ì´ˆ\n"
            if video_context.get('detected_objects'):
                prompt += f"- ê°ì§€ëœ ê°ì²´: {', '.join(video_context['detected_objects'][:5])}\n"
            if video_context.get('ocr_text'):
                prompt += f"- ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {video_context['ocr_text']}\n"
        
        prompt += "\nì£¼ì–´ì§„ ì˜ìƒì„ ë¶„ì„í•˜ê³  ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”."
        
        return prompt
    
    def _calculate_confidence(self, response_text):
        """ì‘ë‹µ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not response_text:
            return 0.0
        
        confidence_keywords = ["í™•ì‹¤", "ëª…í™•", "ë¶„ëª…", "í™•ì¸", "ì •í™•", "í™•ì‹¤íˆ"]
        uncertainty_keywords = ["ì•„ë§ˆ", "ì¶”ì¸¡", "ê°€ëŠ¥ì„±", "ë³´ì„", "ê²ƒ ê°™ìŒ", "ì¶”ì •"]
        
        confidence_score = 0.5  # ê¸°ë³¸ê°’
        
        text_lower = response_text.lower()
        
        for keyword in confidence_keywords:
            if keyword in text_lower:
                confidence_score += 0.1
        
        for keyword in uncertainty_keywords:
            if keyword in text_lower:
                confidence_score -= 0.1
        
        # ì‘ë‹µ ê¸¸ì´ ê³ ë ¤
        if len(response_text) < 50:
            confidence_score -= 0.2
        elif len(response_text) > 200:
            confidence_score += 0.1
        
        return max(0.0, min(1.0, confidence_score))
    
    def compare_responses(self, responses: Dict[str, LLMResponse]) -> Dict[str, Any]:
        """ì—¬ëŸ¬ LLM ì‘ë‹µ ë¹„êµ ë¶„ì„"""
        
        valid_responses = [r for r in responses.values() if r.success and r.response_text]
        
        comparison_result = {
            "responses": responses,
            "comparison": {
                "response_count": len(valid_responses),
                "total_models": len(responses),
                "success_rate": len(valid_responses) / len(responses) if responses else 0,
                "average_confidence": 0.0,
                "processing_times": {},
                "similarities": {},
                "best_response": None,
                "consensus": None,
                "recommendation": {}
            }
        }
        
        if not valid_responses:
            comparison_result["comparison"]["consensus"] = "failed"
            comparison_result["comparison"]["recommendation"] = {
                "reliability": "none",
                "message": "ëª¨ë“  AI ëª¨ë¸ì´ ì‘ë‹µì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "recommended_action": "retry"
            }
            return comparison_result
        
        # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
        comparison_result["comparison"]["average_confidence"] = \
            sum(r.confidence_score for r in valid_responses) / len(valid_responses)
        
        # ì²˜ë¦¬ ì‹œê°„ ë¹„êµ
        for model_name, response in responses.items():
            comparison_result["comparison"]["processing_times"][model_name] = response.processing_time
        
        # ìµœê³  ì‘ë‹µ ì„ ì •
        best_response = max(valid_responses, 
                          key=lambda r: r.confidence_score * 0.7 + (len(r.response_text) / 1000) * 0.3)
        comparison_result["comparison"]["best_response"] = best_response.model_name
        
        # ìœ ì‚¬ë„ ë° í•©ì˜ë„ ê³„ì‚°
        if len(valid_responses) >= 2:
            similarity_score = self._calculate_response_similarity(valid_responses)
            comparison_result["comparison"]["similarities"]["average"] = similarity_score
            
            if similarity_score > 0.7:
                comparison_result["comparison"]["consensus"] = "high"
            elif similarity_score > 0.4:
                comparison_result["comparison"]["consensus"] = "medium"
            else:
                comparison_result["comparison"]["consensus"] = "low"
        else:
            comparison_result["comparison"]["consensus"] = "single"
        
        # ì¶”ì²œ ìƒì„±
        comparison_result["comparison"]["recommendation"] = self._generate_recommendation(
            comparison_result["comparison"]
        )
        
        return comparison_result
    
    def _calculate_response_similarity(self, responses: List[LLMResponse]) -> float:
        """ì‘ë‹µ ê°„ ìœ ì‚¬ë„ ê³„ì‚°"""
        if len(responses) < 2:
            return 1.0
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
        response_keywords = []
        for response in responses:
            keywords = set(response.response_text.lower().split())
            # ë¶ˆìš©ì–´ ì œê±°
            stop_words = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 'í•œ', 'í•˜ëŠ”', 'ìˆëŠ”', 'ê°™ì€'}
            keywords = keywords - stop_words
            response_keywords.append(keywords)
        
        similarities = []
        for i in range(len(response_keywords)):
            for j in range(i + 1, len(response_keywords)):
                intersection = len(response_keywords[i] & response_keywords[j])
                union = len(response_keywords[i] | response_keywords[j])
                if union > 0:
                    similarities.append(intersection / union)
        
        return sum(similarities) / len(similarities) if similarities else 0.0
    
    def _generate_recommendation(self, comparison_data):
        """ì¶”ì²œ ìƒì„±"""
        consensus = comparison_data["consensus"]
        success_rate = comparison_data["success_rate"]
        avg_confidence = comparison_data["average_confidence"]
        
        if success_rate < 0.5:
            return {
                "reliability": "low",
                "message": "ëŒ€ë¶€ë¶„ì˜ AI ëª¨ë¸ì´ ì‘ë‹µì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.",
                "recommended_action": "retry"
            }
        elif consensus == "high" and avg_confidence > 0.7:
            return {
                "reliability": "high",
                "message": "ëª¨ë“  AI ëª¨ë¸ì´ ë†’ì€ ì‹ ë¢°ë„ë¡œ ìœ ì‚¬í•œ ë‹µë³€ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.",
                "recommended_action": "accept"
            }
        elif consensus == "medium":
            return {
                "reliability": "medium",
                "message": "AI ëª¨ë¸ë“¤ ê°„ì— ì¼ë¶€ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ í™•ì¸ì´ ë„ì›€ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "recommended_action": "review"
            }
        else:
            return {
                "reliability": "low",
                "message": "AI ëª¨ë¸ë“¤ì˜ ë‹µë³€ì´ í¬ê²Œ ë‹¤ë¦…ë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.",
                "recommended_action": "clarify"
            }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_multi_llm_analyzer = None

def get_multi_llm_analyzer():
    """ì „ì—­ MultiLLMVideoAnalyzer ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _multi_llm_analyzer
    if _multi_llm_analyzer is None:
        _multi_llm_analyzer = MultiLLMVideoAnalyzer()
    return _multi_llm_analyzer